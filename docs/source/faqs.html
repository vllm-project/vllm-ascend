<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>FAQs</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h1 id="faqs">FAQs</h1>
<h2 id="version-specific-faqs">Version Specific FAQs</h2>
<ul>
<li><a href="https://github.com/vllm-project/vllm-ascend/issues/1007">[v0.7.3.post1] FAQ &amp; Feedback</a></li>
<li><a href="https://github.com/vllm-project/vllm-ascend/issues/1115">[v0.9.0rc2] FAQ &amp; Feedback</a></li>
</ul>
<h2 id="general-faqs">General FAQs</h2>
<h3 id="1-what-devices-are-currently-supported">1. What devices are currently supported?</h3>
<p>Currently, <strong>ONLY Atlas A2 series</strong>  (Ascend-cann-kernels-910b) are supported:</p>
<ul>
<li>Atlas A2 Training series (Atlas 800T A2, Atlas 900 A2 PoD, Atlas 200T A2 Box16, Atlas 300T A2)</li>
<li>Atlas 800I A2 Inference series (Atlas 800I A2)</li>
</ul>
<p>Below series are NOT supported yet:</p>
<ul>
<li>Atlas 300I Duo„ÄÅAtlas 300I Pro (Ascend-cann-kernels-310p) might be supported on 2025.Q2</li>
<li>Atlas 200I A2 (Ascend-cann-kernels-310b) unplanned yet</li>
<li>Ascend 910, Ascend 910 Pro B (Ascend-cann-kernels-910) unplanned yet</li>
</ul>
<p>From a technical view, vllm-ascend support would be possible if the torch-npu is supported. Otherwise, we have to implement it by using custom ops. We are also welcome to join us to improve together.</p>
<h3 id="2-how-to-get-our-docker-containers">2. How to get our docker containers?</h3>
<p>You can get our containers at <code>Quay.io</code>, e.g., <a href="https://quay.io/repository/ascend/vllm-ascend?tab=tags"><u>vllm-ascend</u></a> and <a href="https://quay.io/repository/ascend/cann?tab=tags"><u>cann</u></a>.</p>
<p>If you are in China, you can use <code>daocloud</code> to accelerate your downloading:</p>
<pre><code class="language-bash"><span class="hljs-comment"># Replace with tag you want to pull</span>
TAG=v0.7.3rc2
docker pull m.daocloud.io/quay.io/ascend/vllm-ascend:<span class="hljs-variable">$TAG</span>
</code></pre>
<h3 id="3-what-models-does-vllm-ascend-supports">3. What models does vllm-ascend supports?</h3>
<p>Find more details <a href="https://vllm-ascend.readthedocs.io/en/latest/user_guide/supported_models.html"><u>here</u></a>.</p>
<h3 id="4-how-to-get-in-touch-with-our-community">4. How to get in touch with our community?</h3>
<p>There are many channels that you can communicate with our community developers / users:</p>
<ul>
<li>Submit a GitHub <a href="https://github.com/vllm-project/vllm-ascend/issues?page=1"><u>issue</u></a>.</li>
<li>Join our <a href="https://docs.google.com/document/d/1hCSzRTMZhIB8vRq1_qOOjx4c9uYUxvdQvDsMV2JcSrw/edit?tab=t.0#heading=h.911qu8j8h35z"><u>weekly meeting</u></a> and share your ideas.</li>
<li>Join our <a href="https://github.com/vllm-project/vllm-ascend/issues/227"><u>WeChat</u></a> group and ask your quenstions.</li>
<li>Join our ascend channel in <a href="https://discuss.vllm.ai/c/hardware-support/vllm-ascend-support/6"><u>vLLM forums</u></a> and publish your topics.</li>
</ul>
<h3 id="5-what-features-does-vllm-ascend-v1-supports">5. What features does vllm-ascend V1 supports?</h3>
<p>Find more details <a href="https://github.com/vllm-project/vllm-ascend/issues/414"><u>here</u></a>.</p>
<h3 id="6-how-to-solve-the-problem-of-failed-to-infer-device-type-or-libatbso-cannot-open-shared-object-file">6. How to solve the problem of &quot;Failed to infer device type&quot; or &quot;<a href="http://libatb.so">libatb.so</a>: cannot open shared object file&quot;?</h3>
<p>Basically, the reason is that the NPU environment is not configured correctly. You can:</p>
<ol>
<li>try <code>source /usr/local/Ascend/nnal/atb/set_env.sh</code> to enable NNAL package.</li>
<li>try <code>source /usr/local/Ascend/ascend-toolkit/set_env.sh</code> to enable CANN package.</li>
<li>try <code>npu-smi info</code> to check whether the NPU is working.</li>
</ol>
<p>If all above steps are not working, you can try the following code with python to check whether there is any error:</p>
<pre><code>import torch
import torch_npu
import vllm
</code></pre>
<p>If all above steps are not working, feel free to submit a GitHub issue.</p>
<h3 id="7-how-does-vllm-ascend-perform">7. How does vllm-ascend perform?</h3>
<p>Currently, only some models are improved. Such as <code>Qwen2 VL</code>, <code>Deepseek  V3</code>. Others are not good enough. From 0.9.0rc2, Qwen and Deepseek works with graph mode to play a good performance. What's more, you can install <code>mindie-turbo</code> with <code>vllm-ascend v0.7.3</code> to speed up the inference as well.</p>
<h3 id="8-how-vllm-ascend-work-with-vllm">8. How vllm-ascend work with vllm?</h3>
<p>vllm-ascend is a plugin for vllm. Basically, the version of vllm-ascend is the same as the version of vllm. For example, if you use vllm 0.7.3, you should use vllm-ascend 0.7.3 as well. For main branch, we will make sure <code>vllm-ascend</code> and <code>vllm</code> are compatible by each commit.</p>
<h3 id="9-does-vllm-ascend-support-prefill-disaggregation-feature">9. Does vllm-ascend support Prefill Disaggregation feature?</h3>
<p>Currently, only 1P1D is supported on V0 Engine. For V1 Engine or NPND support, We will make it stable and supported by vllm-ascend in the future.</p>
<h3 id="10-does-vllm-ascend-support-quantization-method">10. Does vllm-ascend support quantization method?</h3>
<p>Currently, w8a8 quantization is already supported by vllm-ascend originally on v0.8.4rc2 or higher, If you're using vllm 0.7.3 version, w8a8 quantization is supporeted with the integration of vllm-ascend and mindie-turbo, please use <code>pip install vllm-ascend[mindie-turbo]</code>.</p>
<h3 id="11-how-to-run-w8a8-deepseek-model">11. How to run w8a8 DeepSeek model?</h3>
<p>Please following the <a href="https://vllm-ascend.readthedocs.io/en/main/tutorials/multi_npu_quantization.html">quantization inferencing tutorail</a> and replace model to DeepSeek.</p>
<h3 id="12-there-is-not-output-in-log-when-loading-models-using-vllm-ascend-how-to-solve-it">12. There is not output in log when loading models using vllm-ascend, How to solve it?</h3>
<p>If you're using vllm 0.7.3 version, this is a known progress bar display issue in VLLM, which has been resolved in <a href="https://github.com/vllm-project/vllm/pull/12428">this PR</a>, please cherry-pick it locally by yourself. Otherwise, please fill up an issue.</p>
<h3 id="13-how-vllm-ascend-is-tested">13. How vllm-ascend is tested</h3>
<p>vllm-ascend is tested by functional test, performance test and accuracy test.</p>
<ul>
<li>
<p><strong>Functional test</strong>: we added CI, includes portion of vllm's native unit tests and vllm-ascend's own unit testsÔºåon vllm-ascend's test, we test basic functionality„ÄÅpopular models availability and <a href="https://vllm-ascend.readthedocs.io/en/latest/user_guide/suppoted_features.html">supported features</a> via e2e test</p>
</li>
<li>
<p><strong>Performance test</strong>: we provide <a href="https://github.com/vllm-project/vllm-ascend/tree/main/benchmarks">benchmark</a> tools for end-to-end performance benchmark which can easily to re-route locally, we'll publish a perf website like <a href="https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/perf">vllm</a> does to show the performance test results for each pull request</p>
</li>
<li>
<p><strong>Accuracy test</strong>: we're working on adding accuracy test to CI as well.</p>
</li>
</ul>
<p>Finnall, for each release, we'll publish the performance test and accuracy test report in the future.</p>
<h3 id="14-how-to-fix-the-error-invalidversion-when-using-vllm-ascend">14. How to fix the error &quot;InvalidVersion&quot; when using vllm-ascend?</h3>
<p>It's usually because you have installed an dev/editable version of vLLM package. In this case, we provide the env variable <code>VLLM_VERSION</code> to let users specify the version of vLLM package to use. Please set the env variable <code>VLLM_VERSION</code> to the version of vLLM package you have installed. The format of <code>VLLM_VERSION</code> should be <code>X.Y.Z</code>.</p>
<h3 id="15-how-to-handle-out-of-memory">15. How to handle Out Of Memory?</h3>
<p>OOM errors typically occur when the model exceeds the memory capacity of a single NPU. For general guidance, you can refer to <a href="https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#out-of-memory">vLLM's OOM troubleshooting documentation</a>.</p>
<p>In scenarios where NPUs have limited HBM (High Bandwidth Memory) capacity, dynamic memory allocation/deallocation during inference can exacerbate memory fragmentation, leading to OOM. To address this:</p>
<ul>
<li>
<p><strong>Adjust <code>--gpu-memory-utilization</code></strong>: If unspecified, will use the default value of <code>0.9</code>. You can decrease this param to reserve more memory to reduce fragmentation risks. See more note in: <a href="https://docs.vllm.ai/en/latest/serving/engine_args.html#vllm.engine.arg_utils-_engine_args_parser-cacheconfig">vLLM - Inference and Serving - Engine Arguments</a>.</p>
</li>
<li>
<p><strong>Configure <code>PYTORCH_NPU_ALLOC_CONF</code></strong>: Set this environment variable to optimize NPU memory management. For example, you can <code>export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True</code> to enable virtual memory feature to mitigate memory fragmentation caused by frequent dynamic memory size adjustments during runtime, see more note in: <a href="https://www.hiascend.com/document/detail/zh/Pytorch/700/comref/Envvariables/Envir_012.html">PYTORCH_NPU_ALLOC_CONF</a>.</p>
</li>
</ul>
<h3 id="16-failed-to-enable-npu-graph-mode-when-running-deepseek">16. Failed to enable NPU graph mode when running DeepSeek?</h3>
<p>You may encounter the following error if running DeepSeek with NPU graph mode enabled. The allowed number of queries per kv when enabling both MLA and Graph mode only support {32, 64, 128}, <strong>Thus this is not supported for DeepSeek-V2-Lite</strong>, as it only has 16 attention heads. The NPU graph mode support on DeepSeek-V2-Lite will be done in the future.</p>
<p>And if you're using DeepSeek-V3 or DeepSeek-R1, please make sure after the tensor parallel split, num_heads / num_kv_heads in {32, 64, 128}.</p>
<pre><code class="language-bash">[rank0]: RuntimeError: EZ9999: Inner Error!
[rank0]: EZ9999: [PID: 62938] 2025-05-27-06:52:12.455.807 numHeads / numKvHeads = 8, MLA only support {32, 64, 128}.[FUNC:CheckMlaAttrs][FILE:incre_flash_attention_tiling_check.cc][LINE:1218]
</code></pre>
<h3 id="17-failed-to-reinstall-vllm-ascend-from-source-after-uninstalling-vllm-ascend">17. Failed to reinstall vllm-ascend from source after uninstalling vllm-ascend?</h3>
<p>You may encounter the problem of C compilation failure when reinstalling vllm-ascend from source using pip if the build directory is not cleaned up. So when reinstalling vllm-ascend from source, the build directory needs to be deleted:</p>
<pre><code class="language-bash"><span class="hljs-built_in">cd</span> vllm-ascend
<span class="hljs-built_in">rm</span> -rf build
</code></pre>

            <script async src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
            
        </body>
        </html>