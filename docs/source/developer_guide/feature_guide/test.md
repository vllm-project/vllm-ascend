# MTP

## Why MTP

MTP(Multi Token Prediction) boosts inference performance by parallelizing the prediction of multiple tokens, shifting from single-token to multi-token generation. This approach significantly increases generation throughput and achieves multiplicative acceleration in inference speed—all without compromising output quality.

## How to use MTP

To enable MTP for DeepSeek-V3 models, add the following parameter when starting the service:

`--speculative-config '{"num_speculative_tokens": 1, "method": "deepseek_mtp"}'`

If you want MTP to predict multiple tokens at once, simply adjust the `num_speculative_tokens` value to your desired number. Currently, MTP supports predicting up to 16 tokens simultaneously.

## How it works

The inference process of DeepSeek with MTP consists of distinct phases followed.

### k = 1

#### Predict

Utilizes k independent heads to generate k tokens in parallel, with each head producing one token.

When flash attention is employed, the corresponding token key-value (KV) pairs are cached into the final layer's KV cache.

#### Verify

The original input sequence is concatenated with the newly generated tokens from Phase 1, forming multiple **<sequence_input, label>** pairs.

These pairs are batched together and sent to the Main Model for verification in a single forward pass.

During this phase, the main model processes **(1 + k)** tokens per request.

#### Accept

The system identifies the longest sequence k where the token predicted by Head1 matches the corresponding label from the verification phase.

This matching sequence is accepted as the correct output.

### k > 1

When k is set greater than 1, the core workflow remains largely similar to the k=1 scenario. The key difference lies in the MTP model's forward pass, which now executes iteratively for k cycles.

In our current vLLM-ascend implementation, we've adopted a design where multiple MTP modules share the same KV cache layer. This architectural choice stems from the fact that DeepSeek-V3 has only open-sourced a single layer of MTP weights.

## DFX

During decoding, the main model processes the previous round’s output token and the predicted token together (computing 1+k tokens simultaneously). The first token is always correct, while the second token—referred to as the **bonus token**—is uncertain since it is derived from speculative prediction.

### Acceptance Strategies for Bonus Token

We employ two strategies to determine whether the bonus token should be accepted.

#### Greedy Strategy

Verify whether the token generated by the main model matches the speculative token predicted by MTP in the previous round. If they match exactly, accept the bonus token; otherwise, reject it and any subsequent tokens derived from that speculation

#### Rejection Sampling Strategy

For each draft token, acceptance is determined by the following inequality: **P_target / P_draft >= U**

Where:
- P_target: Probability assigned to the current draft token by the target model
- P_draft: Probability assigned by the draft model
- U: A random number sampled uniformly from [0, 1)

##### Decision Logic

- **Accept**: If P_target / P_draft >= U, accept the draft token as output

- **Reject**: If P_target / P_draft < U, reject the draft token

##### Recovery Sampling After Rejection

If a token is rejected, a "recovered token" is resampled from an adjusted probability distribution: **Q = max(P_target - P_draft, 0)**

In the current MTP implementation, `P_draft` is not provided and defaults to 1, simplifying the formulas to:

- **Acceptance**: `P_target >= U`

- **Recovery Distribution**: `Q = max(P_target - 1, 0)`

#### Post-Validation Token Generation

- If the bonus token is accepted, the MTP model performs inference for 2 tokens (original main model output token  bonus token)

- If rejected, inference is performed for only 1 token

#### Graph Mode Consideration

In graph mode (to maintain static graph constraints), inference always computes 2 tokens regardless of acceptance. However, only the validated token(s) up to the accepted index are retained in the final output to ensure correctness.

## Limitation

In the current vLLM-ascend implementation, we have opted to have multiple MTP modules share the same KV cache layer. This design decision is made because DeepSeek-V3 has only open-sourced a single layer of MTP weights. Consequently, performance gains cannot be guaranteed for scenarios where k>1 (particularly when k>3).