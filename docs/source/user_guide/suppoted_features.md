# Feature Support

The feature support principle of vLLM Ascend is: **aligned with the vLLM**. We are also actively collaborating with the community to accelerate support.

You can check the [support status of vLLM V1 Engine][v1_user_guide]. Below is the feature support status of vLLM Ascend:

| Feature                       | vLLM V0 Engine | vLLM V1 Engine | Next Step                                                              |
|-------------------------------|----------------|----------------|------------------------------------------------------------------------|
| Chunked Prefill               | 游뚾 WIP         | 游뚾 WIP         | Functional, waiting for CANN 8.1 nnal package release                  |
| Automatic Prefix Caching      | 游뚾 WIP         | 游뚾 WIP         | Functional, waiting for CANN 8.1 nnal package release                  |
| LoRA                          | 游릭 Functional  | 游뚾 WIP         | [vllm-ascend#396][multilora], CI needed, working on V1 support         |
| Prompt adapter                | No plan        | 游리 Planned     | Plan in 2025.06.30                                                     |
| Speculative decoding          | 游릭 Functional  | 游뚾 WIP         | CI needed; working on V1 support                                       |
| Pooling                       | 游릭 Functional  | 游릭 Functional  | CI needed and adapting more models; V1 support rely on vLLM support.   |
| Enc-dec                       | 游댮 NO plan     | 游리 Planned     | Plan in 2025.06.30                                                     |
| Multi Modality                | 游릭 Functional  | 游릭 Functional  | [Tutorial][multimodal], optimizing and adapting more models            |
| LogProbs                      | 游릭 Functional  | 游릭 Functional  | CI needed                                                              |
| Prompt logProbs               | 游릭 Functional  | 游릭 Functional  | CI needed                                                              |
| Async output                  | 游릭 Functional  | 游릭 Functional  | CI needed                                                              |
| Multi step scheduler          | 游릭 Functional  | 游댮 Deprecated  | [vllm#8779][v1_rfc], replaced by [vLLM V1 Scheduler][v1_scheduler])    | 
| Best of                       | 游릭 Functional  | 游댮 Deprecated  | [vllm#13361][best_of], CI needed                                       |
| Beam search                   | 游릭 Functional  | 游릭 Functional  | CI needed                                                              |
| Guided Decoding               | 游릭 Functional  | 游릭 Functional  | [vllm-ascend#177][guided_decoding]                                     |
| Tensor Parallel               | 游릭 Functional  | 游릭 Functional  | CI needed                                                              |
| Pipeline Parallel             | 游릭 Functional  | 游릭 Functional  | CI needed                                                              |
| Expert Parallel               | 游댮 NO plan     | 游릭 Functional  | CI needed; No plan on V0 support                                       |
| Data Parallel                 | 游댮 NO plan     | 游릭 Functional  | CI needed;  No plan on V0 support                                      |
| Prefill Decode Disaggregation | 游릭 Functional  | 游릭 Functional  | 1P1D available, working on xPyD and V1 support.                        |
| Quantization                  | 游릭 Functional  | 游릭 Functional  | W8A8 available, CI needed; working on more quantization method support |
| Graph Mode                    | 游댮 NO plan     | 游릭 Functional  | Functional, waiting for CANN 8.1 nnal package release                  |
| Sleep Mode                    | 游릭 Functional  | 游릭 Functional  | level=1 available, CI needed, working on V1 support                    |

- 游릭 Functional: Fully operational, with ongoing optimizations.
- 游뚾 WIP: Under active development
- 游리 Planned: Scheduled for future implementation (some may have open PRs/RFCs).
- 游댮 NO plan / Deprecated: No plan for V0 or deprecated by vLLM v1.

[v1_user_guide]: https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html
[multimodal]: https://vllm-ascend.readthedocs.io/en/latest/tutorials/single_npu_multimodal.html
[best_of]: https://github.com/vllm-project/vllm/issues/13361
[guided_decoding]: https://github.com/vllm-project/vllm-ascend/issues/177
[v1_scheduler]: https://github.com/vllm-project/vllm/blob/main/vllm/v1/core/sched/scheduler.py
[v1_rfc]: https://github.com/vllm-project/vllm/issues/8779
[multilora]: https://github.com/vllm-project/vllm-ascend/issues/396
