# Inference and Serving

This section provides a detailed usage guide of vLLM Ascend features.

:::{toctree}
:caption: Inference and Serving
:maxdepth: 1
offline_inference
openai_compatible_server
tensor_parallel
data_parallel
expert_parallel
sequence_parallel
context_parallel
pipeline_parallel
:::
