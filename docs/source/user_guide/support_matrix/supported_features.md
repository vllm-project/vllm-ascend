# Feature Support

The feature support principle of vLLM Ascend is: **aligned with the vLLM**. We are also actively collaborating with the community to accelerate support.

You can check the [support status of vLLM V1 Engine][v1_user_guide]. Below is the feature support status of vLLM Ascend:

| Feature                       |      Status    | Next Step                                                              |
|-------------------------------|----------------|------------------------------------------------------------------------|
| Chunked Prefill               | 游릭 Functional  | Functional, see detail note: [Chunked Prefill][cp]                     |
| Automatic Prefix Caching      | 游릭 Functional  | Functional, see detail note: [vllm-ascend#732][apc]                    |
| LoRA                          | 游릭 Functional  | [vllm-ascend#396][multilora], [vllm-ascend#893][v1 multilora]          |
| Prompt adapter                | 游댮 No plan     | This feature has been deprecated by vLLM.                              |
| Speculative decoding          | 游릭 Functional  | Basic support                                                          |
| Pooling                       | 游릭 Functional  | CI needed and adapting more models; V1 support rely on vLLM support.   |
| Enc-dec                       | 游리 Planned     | vLLM should support this feature first.                                |
| Multi Modality                | 游릭 Functional  | [Tutorial][multimodal], optimizing and adapting more models            |
| LogProbs                      | 游릭 Functional  | CI needed                                                              |
| Prompt logProbs               | 游릭 Functional  | CI needed                                                              |
| Async output                  | 游릭 Functional  | CI needed                                                              |
| Multi step scheduler          | 游댮 Deprecated  | [vllm#8779][v1_rfc], replaced by [vLLM V1 Scheduler][v1_scheduler]     |
| Best of                       | 游댮 Deprecated  | [vllm#13361][best_of]                                                  |
| Beam search                   | 游릭 Functional  | CI needed                                                              |
| Guided Decoding               | 游릭 Functional  | [vllm-ascend#177][guided_decoding]                                     |
| Tensor Parallel               | 游릭 Functional  | Make TP >4 work with graph mode                                        |
| Pipeline Parallel             | 游릭 Functional  | Write official guide and tutorial.                                     |
| Expert Parallel               | 游릭 Functional  | Dynamic EPLB support.                                                  |
| Data Parallel                 | 游릭 Functional  | Data Parallel support for Qwen3 MoE.                                   |
| Prefill Decode Disaggregation | 游뚾 WIP         | working on [1P1D] and xPyD.                                            |
| Quantization                  | 游릭 Functional  | W8A8 available; working on more quantization method support(W4A8, etc) |
| Graph Mode                    | 游댯 Experimental| Experimental, see detail note: [vllm-ascend#767][graph_mode]           |
| Sleep Mode                    | 游릭 Functional  |                                                                        |

- 游릭 Functional: Fully operational, with ongoing optimizations.
- 游댯 Experimental: Experimental support, interfaces and functions may change.
- 游뚾 WIP: Under active development, will be supported soon.
- 游리 Planned: Scheduled for future implementation (some may have open PRs/RFCs).
- 游댮 NO plan / Deprecated: No plan or deprecated by vLLM.

[v1_user_guide]: https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html
[multimodal]: https://vllm-ascend.readthedocs.io/en/latest/tutorials/single_npu_multimodal.html
[best_of]: https://github.com/vllm-project/vllm/issues/13361
[guided_decoding]: https://github.com/vllm-project/vllm-ascend/issues/177
[v1_scheduler]: https://github.com/vllm-project/vllm/blob/main/vllm/v1/core/sched/scheduler.py
[v1_rfc]: https://github.com/vllm-project/vllm/issues/8779
[multilora]: https://github.com/vllm-project/vllm-ascend/issues/396
[v1 multilora]: https://github.com/vllm-project/vllm-ascend/pull/893
[graph_mode]: https://github.com/vllm-project/vllm-ascend/issues/767
[apc]: https://github.com/vllm-project/vllm-ascend/issues/732
[cp]: https://docs.vllm.ai/en/stable/performance/optimization.html#chunked-prefill
[1P1D]: https://github.com/vllm-project/vllm-ascend/pull/950
[ray]: https://github.com/vllm-project/vllm-ascend/issues/1751
