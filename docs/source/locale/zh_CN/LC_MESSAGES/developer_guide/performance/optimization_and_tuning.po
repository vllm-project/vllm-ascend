# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-08-30 16:51+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/developer_guide/performance/optimization_and_tuning.md:1
msgid "Optimization and Tuning"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:3
msgid ""
"This guide aims to help users to improve vllm-ascend performance on "
"system level. It includes OS configuration, library optimization, deploy "
"guide and so on. Any feedback is welcome."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:5
msgid "Preparation"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:7
msgid "Run the container:"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:30
msgid "Configure your environment:"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:48
msgid "Install vllm and vllm-ascend:"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:60
msgid ""
"Please follow the [Installation Guide](https://vllm-"
"ascend.readthedocs.io/en/latest/installation.html) to make sure vllm, "
"vllm-ascend and mindie-turbo is installed correctly."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:63
msgid ""
"Make sure your vllm and vllm-ascend are installed after your python "
"configuration completed, because these packages will build binary files "
"using the python in current environment. If you install vllm, vllm-ascend"
" and mindie-turbo before chapter 1.1, the binary files will not use the "
"optimized python."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:66
msgid "Optimizations"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:68
msgid "1. Compilation Optimization"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:70
msgid "1.1. Install optimized `python`"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:72
msgid ""
"Python supports **LTO** and **PGO** optimization starting from version "
"`3.6` and above, which can be enabled at compile time. And we have "
"offered compilation optimized `python` packages directly to users for the"
" sake of convenience. You can also reproduce the `python` build follow "
"this "
"[tutorial](https://www.hiascend.com/document/detail/zh/Pytorch/600/ptmoddevg/trainingmigrguide/performance_tuning_0063.html)"
" according to your specific scenarios."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:100
msgid "2. OS Optimization"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:102
msgid "2.1. jemalloc"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:104
msgid ""
"**jemalloc** is a memory allocator that improves performance for multi-"
"threads scenario and can reduce memory fragment. jemalloc use thread "
"local memory manager to allocate variables, which can avoid lock "
"competition between multi-threads and can hugely optimize performance."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:116
msgid "2.2. Tcmalloc"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:118
msgid ""
"**Tcmalloc (Thread Counting Malloc)** is a universal memory allocator "
"that improves overall performance while ensuring low latency by "
"introducing a multi-level cache structure, reducing mutex competition and"
" optimizing large object processing flow. Find more details "
"[here](https://www.hiascend.com/document/detail/zh/Pytorch/700/ptmoddevg/trainingmigrguide/performance_tuning_0068.html)."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:139
msgid "3. `torch_npu` Optimization"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:141
msgid ""
"Some performance tuning features in `torch_npu` are controlled by "
"environment variables. Some features and their related environment "
"variables are shown below."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:143
msgid "Memory optimization:"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:154
msgid "Schedule optimization:"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:165
msgid "4. CANN Optimization"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:167
msgid "4.1. HCCL Optimization"
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:169
msgid ""
"There are some performance tuning features in HCCL, which are controlled "
"by environment variables."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:171
msgid ""
"You can configure HCCL to use \"AIV\" mode to optimize performance by "
"setting the environment variable shown below. In \"AIV\" mode, the "
"communication is scheduled by AI vector core directly with ROCE, instead "
"of being scheduled by AI cpu."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:178
msgid ""
"Plus, there are more features for performance optimization in specific "
"scenarios, which are shown below."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:180
msgid ""
"`HCCL_INTRA_ROCE_ENABLE`: Use RDMA link instead of SDMA link between two "
"8Ps as the mesh interconnect link, find more details "
"[here](https://www.hiascend.com/document/detail/zh/Pytorch/600/ptmoddevg/trainingmigrguide/performance_tuning_0044.html)."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:181
msgid ""
"`HCCL_RDMA_TC`: Use this var to configure traffic class of RDMA network "
"card, find more details "
"[here](https://www.hiascend.com/document/detail/zh/Pytorch/600/ptmoddevg/trainingmigrguide/performance_tuning_0045.html)."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:182
msgid ""
"`HCCL_RDMA_SL`: Use this var to configure service level of RDMA network "
"card, find more details "
"[here](https://www.hiascend.com/document/detail/zh/Pytorch/600/ptmoddevg/trainingmigrguide/performance_tuning_0046.html)."
msgstr ""

#: ../../source/developer_guide/performance/optimization_and_tuning.md:183
msgid ""
"`HCCL_BUFFSIZE`: Use this var to control the cache size for sharing data "
"between two NPUs, find more details "
"[here](https://www.hiascend.com/document/detail/zh/Pytorch/600/ptmoddevg/trainingmigrguide/performance_tuning_0047.html)."
msgstr ""

