# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-17 12:01+0800\n"
"PO-Revision-Date: 2025-12-17 14:30+0800\n"
"Last-Translator: vLLM-Ascend Team <contact@example.com>\n"
"Language: zh_CN\n"
"Language-Team: Chinese (Simplified) <contact@example.com>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:1
msgid "ACL Graph"
msgstr "ACL图"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:3
msgid "Why we need ACL Graph?"
msgstr "为什么需要ACL图？"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:5
msgid ""
"When in LLM inference, each token requires nearly thousand operator "
"executions, and when host launching operators are slower than device, it "
"will cause host bound. In severe cases, the device will be idle for more "
"than half of the time. To solve this problem, we use graph in LLM "
"inference."
msgstr ""
"在大语言模型推理时，每个令牌需要近千次算子执行。当主机侧（CPU）的算子启动速度慢于设备侧（NPU）的计算速度时，会造成主机瓶颈（Host Bound）。在严重情况下，设备侧会有一半以上的时间处于空闲状态。为了解决这个问题，我们在LLM推理中使用了图模式。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:26
msgid "How to use ACL Graph?"
msgstr "如何使用ACL图？"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:28
msgid ""
"ACL Graph is enabled by default in V1 Engine, just need to check that "
"`enforce_eager` is not set to `True`. More details see: [Graph Mode "
"Guide](https://vllm-"
"ascend.readthedocs.io/en/latest/user_guide/feature_guide/graph_mode.html)"
msgstr ""
"在V1 Engine中，ACL图模式默认启用，只需确保`enforce_eager`没有被设置为`True`。更多详细信息请参阅：[图模式指南](https://vllm-ascend.readthedocs.io/en/latest/user_guide/feature_guide/graph_mode.html)"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:30
msgid "How it works?"
msgstr "它是如何工作的？"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:32
msgid ""
"In short, graph mode works in two steps: **capture and replay**. When "
"engine starts, we will capture all of the ops in model forward and save "
"it as a graph, and when req come in, we just replay the graph on devices,"
" and waiting for result."
msgstr ""
"简而言之，图模式分两步工作：**捕获和重放**。当引擎启动时，我们会捕获模型前向传播中的所有算子并将其保存为一个图。当请求到来时，我们只需在设备上重放该图，并等待结果。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:34
msgid "But in reality, graph mode is not that simple."
msgstr "但实际上，图模式并非如此简单。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:36
msgid "Padding and Bucketing"
msgstr "填充与分桶"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:38
msgid ""
"Due to graph can only replay the ops captured before, without doing "
"tiling and checking graph input, we need to ensure the consistency of the"
" graph input, but we know that model input's shape depends on the request"
" scheduled by Scheduler, we can't ensure the consistency."
msgstr ""
"由于图只能重放之前捕获的算子，无法进行动态分块和检查图输入，我们需要确保图输入的一致性。但我们知道，模型输入的形状取决于调度器（Scheduler）调度的请求，我们无法保证这种一致性。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:40
msgid ""
"Obviously, we can solve this problem by capturing the biggest shape and "
"padding all of the model input to it. But it will bring a lot of "
"redundant computing and make performance worse. So we can capture "
"multiple graphs with different shape, and pad the model input to the "
"nearest graph, which will greatly reduce redundant computing. But when "
"`max_num_batched_tokens` is very large, the number of graphs that need to"
" be captured will also become very large. But we know that when "
"intensor's shape is large, the computing time will be very long, and "
"graph mode is not necessary in this case. So all of things we need to do "
"is:"
msgstr ""
"显然，我们可以通过捕获最大形状并将所有模型输入填充到该形状来解决这个问题。但这会带来大量冗余计算并导致性能下降。因此，我们可以捕获多个不同形状的图，并将模型输入填充到最接近的图，这将大大减少冗余计算。但是，当`max_num_batched_tokens`非常大时，需要捕获的图数量也会变得非常大。但我们知道，当输入张量形状很大时，计算时间会很长，此时图模式并非必需。因此，我们需要做的是："

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:41
msgid "Set a threshold;"
msgstr "设置一个阈值；"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:42
msgid ""
"When `num_scheduled_tokens` is bigger than the threshold, use "
"`eager_mode`;"
msgstr "当`num_scheduled_tokens`大于阈值时，使用`eager_mode`；"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:43
msgid "Capture multiple graphs within a range below the threshold;"
msgstr "在阈值以下的范围内捕获多个图；"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:58
msgid "Piecewise and Full graph"
msgstr "分段图与完整图"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:60
msgid ""
"Due to the increasing complexity of the attention layer in current LLM, "
"we can't ensure all types of attention can run in graph. In MLA, "
"prefill_tokens and decode_tokens have different calculation method, so "
"when a batch has both prefills and decodes in MLA, graph mode is "
"difficult to handle this situation."
msgstr ""
"由于当前LLM中注意力层的复杂性不断增加，我们无法确保所有类型的注意力都可以在图中运行。在MLA（Multi-Head Latent Attention）中，预填充令牌和解码令牌具有不同的计算方法，因此当一个批次中同时包含预填充和解码请求时，图模式难以处理这种情况。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:62
msgid ""
"vLLM solves this problem with piecewise graph mode. We use eager mode to "
"launch attention's ops, and use graph to deal with others. But it also "
"bring some problems: The cost of launching ops has become large again, "
"although much smaller than eager mode, but it will also lead to host "
"bound when cpu is poor or `num_tokens` is small."
msgstr ""
"vLLM通过分段图模式解决了这个问题。我们使用Eager模式启动注意力相关的算子，而用图处理其他部分。但这也会带来一些问题：启动算子的开销再次变大（尽管比完整的Eager模式小很多），当CPU性能较差或`num_tokens`较小时，仍可能导致主机瓶颈。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:64
msgid "Altogether, we need to support both piecewise and full graph mode."
msgstr "总而言之，我们需要同时支持分段图和完整图模式。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:66
msgid ""
"When attention can run in graph, we tend to choose full graph mode to "
"achieve optimal performance;"
msgstr "当注意力可以在图中运行时，我们倾向于选择完整图模式以获得最佳性能；"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:67
msgid "When full graph is not work, use piecewise graph as a substitute;"
msgstr "当完整图不可用时，使用分段图作为替代方案；"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:68
msgid ""
"When piecewise graph's performance is not good and full graph mode is "
"blocked, separate prefills and decodes, and use full graph mode in "
"**decode_only** situation. Because when a batch include prefill req, "
"usually `num_tokens` will be quite big and not cause host bound."
msgstr ""
"当分段图性能不佳且完整图模式被阻塞时，将预填充和解码请求分离，并在**仅解码**的情况下使用完整图模式。因为当一个批次包含预填充请求时，通常`num_tokens`会相当大，不会造成主机瓶颈。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:70
msgid ""
"Currently, due to stream resource constraint, we can only support a few "
"buckets in piecewise graph mode now, which will cause redundant computing"
" and may lead to performance degradation compared with eager mode."
msgstr ""
"目前，由于流资源限制，我们在分段图模式下只能支持少量分桶，这会导致冗余计算，并可能使得性能低于Eager模式。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:72
msgid "How it be implemented?"
msgstr "它是如何实现的？"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:74
msgid ""
"vLLM has already implemented most of the modules in graph mode. You can "
"see more details at: [CUDA "
"Graphs](https://docs.vllm.ai/en/latest/design/cuda_graphs.html)"
msgstr ""
"vLLM已经在图模式中实现了大部分模块。更多详细信息请参阅：[CUDA图](https://docs.vllm.ai/en/latest/design/cuda_graphs.html)"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:76
msgid ""
"When in graph mode, vLLM will call "
"`current_platform.get_static_graph_wrapper_cls` to get current device's "
"graph model wrapper, so what we need to do is to implement the graph mode"
" wrapper on Ascend: `ACLGraphWrapper`."
msgstr ""
"在图模式下，vLLM会调用`current_platform.get_static_graph_wrapper_cls`来获取当前设备的图模型包装器。因此，我们需要在昇腾平台上实现图模式包装器：`ACLGraphWrapper`。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:78
msgid ""
"vLLM has added `support_torch_compile` decorator to all models, this "
"decorator will replace the `__init__` and `forward` interface of the "
"model class, and when `forward` called, the code inside the "
"`ACLGraphWrapper` will be executed, and it will do capture or replay as "
"mentioned above."
msgstr ""
"vLLM已为所有模型添加了`support_torch_compile`装饰器。该装饰器会替换模型类的`__init__`和`forward`接口。当调用`forward`时，将执行`ACLGraphWrapper`内部的代码，并执行如上所述的捕获或重放操作。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:80
msgid ""
"When use piecewise graph, we just need to follow the above-mentioned "
"process, but when in full graph, due to the complexity of the attention, "
"sometimes we need to update attention op's param before execution. So we "
"implement `update_attn_params` and `update_mla_attn_params` funcs for "
"full graph mode. And when forward, memory will be reused between "
"different ops, so we can't update attention op's param before forward. In"
" ACL Graph, we use `torch.npu.graph_task_update_begin` and "
"`torch.npu.graph_task_update_end` to do it, and use "
"`torch.npu.ExternalEvent` to ensure order between params update and ops "
"execution."
msgstr ""
"使用分段图时，我们只需遵循上述流程。但在完整图模式下，由于注意力的复杂性，有时我们需要在执行前更新注意力算子的参数。因此，我们为完整图模式实现了`update_attn_params`和`update_mla_attn_params`函数。在前向传播时，不同算子之间会复用内存，因此我们无法在前向传播之前更新注意力算子的参数。在ACL图中，我们使用`torch.npu.graph_task_update_begin`和`torch.npu.graph_task_update_end`来实现这一点，并使用`torch.npu.ExternalEvent`来确保参数更新和算子执行之间的顺序。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:82
msgid "DFX"
msgstr "可调试性/可观测性（DFX）"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:84
msgid "Stream resource constraint"
msgstr "流资源限制"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:86
msgid ""
"Currently, we can only capture 1800 graphs at most, due to the limitation"
" of ACL graph that a graph requires a separate stream at least. This "
"number is bounded by the number of streams, which is 2048, we save 248 "
"streams as a buffer. Besides, there are many variables that can affect "
"the number of buckets:"
msgstr ""
"目前，由于ACL图的限制（每个图至少需要一个独立的流），我们最多只能捕获1800个图。这个数字受限于流的总数（2048个），我们保留了248个流作为缓冲。此外，有许多因素会影响分桶数量："

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:88
msgid ""
"Piecewise graph will divides the model into `num_hidden_layers + 1` sub "
"modules, based on attention layer. Every sub module is a single graph "
"which need to cost stream, so the number of buckets in piecewise graph "
"mode is very tight compared with full graph mode."
msgstr ""
"分段图会根据注意力层将模型划分为`num_hidden_layers + 1`个子模块。每个子模块都是一个单独的图，需要消耗流资源。因此，与完整图模式相比，分段图模式下的分桶数量非常紧张。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:90
msgid ""
"The number of streams required for a graph is related to the number of "
"comm domains. Each comm domain will increase one stream consumed by a "
"graph."
msgstr "图所需的流数量与通信域（comm domains）的数量有关。每个通信域都会增加一个图所消耗的流。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:92
msgid ""
"When multi-stream is explicitly called in sub module, it will consumes an"
" additional stream."
msgstr "当在子模块中显式调用多流（multi-stream）时，会消耗额外的流。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:94
msgid ""
"There are some other rules about ACL Graph and stream. Currently, we use "
"func `update_aclgraph_sizes` to calculate the maximum number of buckets "
"and update `graph_batch_sizes` to ensure stream resource is sufficient."
msgstr ""
"关于ACL图和流还有一些其他规则。目前，我们使用`update_aclgraph_sizes`函数来计算最大分桶数，并更新`graph_batch_sizes`以确保流资源充足。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:96
msgid "We will expand the stream resource limitation in the future."
msgstr "我们将在未来扩展流资源限制。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:98
msgid "Limitation"
msgstr "当前限制"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:100
msgid "`FULL` and `FULL_AND_PIECEWISE` are not supported now;"
msgstr "目前不支持`FULL`和`FULL_AND_PIECEWISE`模式；"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:101
msgid ""
"When use ACL Graph and MTP and `num_speculative_tokens > 1`, as vLLM "
"don't support this case in v0.11.0, we need to set "
"`cudagraph_capture_sizes` explicitly."
msgstr ""
"当同时使用ACL图、MTP（多任务并行）且`num_speculative_tokens > 1`时，由于vLLM v0.11.0不支持此情况，我们需要显式设置`cudagraph_capture_sizes`。"

#: ../../source/developer_guide/feature_guide/ACL_Graph.md:102
msgid "`use_inductor` is not supported now;"
msgstr "目前不支持`use_inductor`；"