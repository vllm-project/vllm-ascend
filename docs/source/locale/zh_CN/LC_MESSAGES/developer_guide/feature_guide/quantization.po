# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/developer_guide/feature_guide/quantization.md:1
msgid "Quantization Adaptation Guide"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:3
msgid ""
"This document provides guidance for adapting quantization algorithms and "
"models related to **ModelSlim**."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:5
msgid "Quantization Feature Introduction"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:7
msgid "Quantization Inference Process"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:9
msgid ""
"The current process for registering and obtaining quantization methods in"
" vLLM Ascend is as follows:"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:11
msgid "![get_quant_method](../../assets/quantization/get_quant_method.png)"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:11
msgid "get_quant_method"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:13
msgid ""
"vLLM Ascend registers a custom ascend quantization method. By configuring"
" the `--quantization ascend` parameter (or `quantization=\"ascend\"` for "
"offline), the quantization feature is enabled. When constructing the "
"`quant_config`, the registered `AscendQuantConfig` is initialized and "
"`get_quant_method` is called to obtain the quantization method "
"corresponding to each weight part, stored in the `quant_method` "
"attribute."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:15
msgid ""
"Currently supported quantization methods include `AscendLinearMethod`, "
"`AscendFusedMoEMethod`, `AscendEmbeddingMethod`, and their corresponding "
"non-quantized methods:"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:17
msgid "![quant_methods_overview](../../assets/quantization/quant_methods_overview.png)"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:17
msgid "quant_methods_overview"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:19
msgid ""
"The quantization method base class defined by vLLM  and the overall call "
"flow of quantization methods are as follows:"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:21
msgid "![quant_method_call_flow](../../assets/quantization/quant_method_call_flow.png)"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:21
msgid "quant_method_call_flow"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:23
msgid ""
"The `embedding` method is generally not implemented for quantization, "
"focusing only on the other three methods."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:25
msgid ""
"The `create_weights` method is used for weight initialization; the "
"`process_weights_after_loading` method is used for weight post-"
"processing, such as transposition, format conversion, data type "
"conversion, etc.; the `apply` method is used to perform activation "
"quantization and quantized matrix multiplication calculations during the "
"forward process."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:27
msgid ""
"We need to implement the `create_weights`, "
"`process_weights_after_loading`, and `apply` methods for different "
"**layers** (**attention**, **mlp**, **moe**)."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:29
msgid ""
"**Supplemnet**: When loading the model, the quantized model's description"
" file **quant_model_description.json** needs to be read. This file "
"describes the quantization configuration and parameters for each part of "
"the model weights, for example:"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:49
msgid ""
"Based on the above content, we present a brief description of the "
"adaptation process for quantization algorithms and quantized models."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:51
msgid "Quantization Algorithm Adaptation"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:53
msgid ""
"**Step 1: Algorithm Design**. Define the algorithm ID (e.g., "
"`W4A8_DYNAMIC`), determine supported layers (linear, moe, attention), and"
" design the quantization scheme (static/dynamic, "
"pertensor/perchannel/pergroup)."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:54
msgid ""
"**Step 2: Registration**. Add the algorithm ID to "
"`ASCEND_QUANTIZATION_METHOD_MAP` in `vllm_ascend/quantization/utils.py` "
"and associate it with the corresponding method class."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:65
msgid ""
"**Step 3: Implementation**. Create an algorithm implementation file, such"
" as `vllm_ascend/quantization/w4a8_dynamic.py`, and implement the method "
"class and logic."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:66
msgid ""
"**Step 4: Testing**. Use your algorithm to generate quantization "
"configurations and verify correctness and performance on target models "
"and hardware."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:68
msgid "Quantized Model Adaptation"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:70
msgid ""
"Adapting a new quantized model requires ensuring the following three "
"points:"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:72
msgid "The original model has been successfully adapted in `vLLM Ascend`."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:73
msgid ""
"**Fused Module Mapping**: Add the model's `model_type` to "
"`packed_modules_model_mapping` in "
"`vllm_ascend/quantization/quant_config.py` (e.g., `qkv_proj`, "
"`gate_up_proj`, `experts`) to ensure sharding consistency and correct "
"loading."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:93
msgid ""
"All quantization algorithms used by the quantized model have been "
"integrated into the `quantization` module."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:95
msgid "Currently Supported Quantization Algorithms"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:97
msgid ""
"vLLM Ascend supports multiple quantization algorithms. The following "
"table provides an overview of each quantization algorithm based on the "
"implementation in the `vllm_ascend.quantization` module:"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Algorithm"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Weight"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Activation"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Weight Granularity"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Activation Granularity"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Type"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Description"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W4A16`"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "INT4"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "FP16/BF16"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Per-Group"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Per-Tensor"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Static"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"4-bit weight quantization with 16-bit activation precision, specifically "
"designed for MoE model expert layers, supporting int32 format weight "
"packing"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W8A16`"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "INT8"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Per-Channel"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"8-bit weight quantization with 16-bit activation precision, balancing "
"accuracy and performance, suitable for linear layers"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W8A8`"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"Static activation quantization, suitable for scenarios requiring high "
"precision"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W8A8_DYNAMIC`"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Per-Token"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Dynamic"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Dynamic activation quantization with per-token scaling factor calculation"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W4A8_DYNAMIC`"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"Supports both direct per-channel quantization to 4-bit and two-step "
"quantization (per-channel to 8-bit then per-group to 4-bit)"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W4A4_FLATQUANT_DYNAMIC`"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"Uses FlatQuant for activation distribution smoothing before 4-bit dynamic"
" quantization, with additional matrix multiplications for precision "
"preservation"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W8A8_MIX`"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Per-Tensor/Token"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Mixed"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"PD Colocation Scenario uses dynamic quantization for both P node and D "
"node; PD Disaggregation Scenario uses dynamic quantization for P node and"
" static for D node"
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:109
msgid ""
"**Static vs Dynamic:** Static quantization uses pre-computed scaling "
"factors with better performance, while dynamic quantization computes "
"scaling factors on-the-fly for each token/activation tensor with higher "
"precision."
msgstr ""

#: ../../source/developer_guide/feature_guide/quantization.md:111
msgid ""
"**Granularity:** Refers to the scope of scaling factor computation (e.g.,"
" per-tensor, per-channel, per-group)."
msgstr ""

