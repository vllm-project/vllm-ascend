# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-17 12:01+0800\n"
"PO-Revision-Date: 2025-12-17 15:30+0800\n"
"Last-Translator: Gemini <support@google.com>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:1
msgid "Multi Token Prediction (MTP)"
msgstr "多 Token 预测 (MTP)"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:3
msgid "Why We Need MTP"
msgstr "为什么我们需要 MTP"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:4
msgid ""
"MTP boosts inference performance by parallelizing the prediction of "
"multiple tokens, shifting from single-token to multi-token generation. "
"This approach significantly increases generation throughput and achieves "
"multiplicative acceleration in inference speed—all without compromising "
"output quality."
msgstr "MTP 通过并行预测多个 Token，将单 Token 生成模式转变为多 Token 生成，从而提升推理性能。这种方法在不损失输出质量的前提下，显著提高了生成吞吐量，并使推理速度实现成倍增长。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:6
msgid "How to Use MTP"
msgstr "如何使用 MTP"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:7
msgid ""
"To enable MTP for DeepSeek-V3 models, add the following parameter when "
"starting the service:"
msgstr "若要为 DeepSeek-V3 模型启用 MTP，请在启动服务时添加以下参数："

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:9
#, python-brace-format
msgid ""
"--speculative_config ' {\"method\": \"mtp\", \"num_speculative_tokens\": "
"1, \"disable_padded_drafter_batch\": False} '"
msgstr "--speculative_config ' {\"method\": \"mtp\", \"num_speculative_tokens\": 1, \"disable_padded_drafter_batch\": False} '"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:11
msgid ""
"`num_speculative_tokens`: The number of speculative tokens which enable "
"model to predict multiple tokens at once, if provided. It will default to"
" the number in the draft model config if present, otherwise, it is "
"required."
msgstr "`num_speculative_tokens`：投机 Token 的数量。如果提供该参数，模型将能够一次预测多个 Token。如果草稿模型（draft model）配置中已有该数值，则默认为配置值，否则必须手动指定。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:12
msgid ""
"`disable_padded_drafter_batch`: Disable input padding for speculative "
"decoding. If set to True, speculative input batches can contain sequences"
" of different lengths, which may only be supported by certain attention "
"backends. This currently only affects the MTP method of speculation, "
"default is False."
msgstr "`disable_padded_drafter_batch`：禁用投机解码的输入填充。如果设置为 True，投机输入 Batch 可以包含不同长度的序列，但这可能仅受某些注意力后端（attention backend）支持。目前该参数仅影响 MTP 投机方法，默认为 False。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:14
msgid "How It Works"
msgstr "工作原理"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:16
msgid "Module Architecture"
msgstr "模块架构"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:27
msgid "**1. sample**"
msgstr "**1. 采样 (Sample)**"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:29
msgid ""
"*rejection_sample.py*: During decoding, the main model processes the "
"previous round’s output token and the predicted token together (computing"
" 1+k tokens simultaneously). The first token is always correct, while the"
" second token—referred to as the **bonus token**—is uncertain since it is"
" derived from speculative prediction, thus We employ **Greedy Strategy** "
"and **Rejection Sampling Strategy** to determine whether the bonus token "
"should be accepted. The module structure consists of an "
"`AscendRejectionSampler` class with a forward method that implements the "
"specific sampling logic."
msgstr "*rejection_sample.py*：在解码过程中，主模型同时处理上一轮的输出 Token 和预测 Token（同时计算 1+k 个 Token）。第一个 Token 始终是正确的，而第二个 Token（称为 **奖励 Token/Bonus Token**）由于来自投机预测，具有不确定性。因此，我们采用**贪婪策略 (Greedy Strategy)** 和**拒绝采样策略 (Rejection Sampling Strategy)** 来确定是否接受该 Bonus Token。该模块包含一个 `AscendRejectionSampler` 类，其 forward 方法实现了具体的采样逻辑。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:37
msgid "**2. spec_decode**"
msgstr "**2. 投机解码 (Spec_decode)**"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:39
msgid ""
"This section encompasses the model preprocessing for spec-decode, "
"primarily structured as follows: it includes loading the model, executing"
" a dummy run, and generating token ids. These steps collectively form the"
" model data construction and forward invocation for a single spec-decode "
"operation."
msgstr "本节涵盖了投机解码的模型预处理，主要结构如下：包括加载模型、执行空运行（dummy run）以及生成 Token ID。这些步骤共同构成了单次投机解码操作的模型数据构建和前向调用。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:40
msgid ""
"*mtp_proposer.py*: Configure vLLM-Ascend to use speculative decoding "
"where proposals are generated by deepseek mtp layer."
msgstr "*mtp_proposer.py*：配置 vLLM-Ascend 使用投机解码，其中候选 Token (Proposals) 由 DeepSeek MTP 层生成。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:52
msgid "Algorithm"
msgstr "算法"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:54
msgid "**1. Reject_Sample**"
msgstr "**1. 拒绝采样 (Reject_Sample)**"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:55
msgid "*Greedy Strategy*"
msgstr "*贪婪策略*"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:57
msgid ""
"Verify whether the token generated by the main model matches the "
"speculative token predicted by MTP in the previous round. If they match "
"exactly, accept the bonus token; otherwise, reject it and any subsequent "
"tokens derived from that speculation."
msgstr "验证主模型生成的 Token 是否与上一轮 MTP 预测的投机 Token 匹配。如果完全匹配，则接受 Bonus Token；否则，拒绝该 Token 以及基于该次投机产生的所有后续 Token。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:59
msgid "*Rejection Sampling Strategy*"
msgstr "*拒绝采样策略*"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:61
msgid "This method introduces stochasticity in rejection sampling."
msgstr "该方法在拒绝采样中引入了随机性。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:63
msgid ""
"For each draft token, acceptance is determined by verifying whether the "
"inequality `P_target / P_draft ≥ U` holds, where `P_target` represents "
"the probability assigned to the current draft token by the target model, "
"`P_draft` denotes the probability assigned by the draft model, and `U` is"
" a random number sampled uniformly from the interval [0, 1)."
msgstr "对于每个草稿 Token，通过验证不等式 `P_target / P_draft ≥ U` 是否成立来确定是否接受。其中 `P_target` 代表目标模型给当前草稿 Token 分配的概率，`P_draft` 表示草稿模型分配的概率，`U` 是从区间 [0, 1) 中均匀采样的随机数。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:65
msgid ""
"The decision logic for each draft token is as follows: if the inequality "
"`P_target / P_draft ≥ U` holds, the draft token is accepted as output; "
"conversely, if `P_target / P_draft < U`, the draft token is rejected."
msgstr "每个草稿 Token 的判定逻辑如下：如果不等式 `P_target / P_draft ≥ U` 成立，则该草稿 Token 被接受并输出；反之，如果 `P_target / P_draft < U`，则该草稿 Token 被拒绝。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:67
msgid ""
"When a draft token is rejected, a recovery sampling process is triggered "
"where a \"recovered token\" is resampled from the adjusted probability "
"distribution defined as `Q = max(P_target - P_draft, 0)`. In the current "
"MTP implementation, since `P_draft` is not provided and defaults to 1, "
"the formulas simplify such that token acceptance occurs when `P_target ≥ "
"U,` and the recovery distribution becomes `Q = max(P_target - 1, 0)`."
msgstr "当一个草稿 Token 被拒绝时，会触发恢复采样过程，从调整后的概率分布 `Q = max(P_target - P_draft, 0)` 中重新采样一个“恢复 Token”。在当前的 MTP 实现中，由于不提供 `P_draft` 且默认为 1，公式简化为：当 `P_target ≥ U` 时接受 Token，恢复分布变为 `Q = max(P_target - 1, 0)`。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:69
msgid "**2. Performance**"
msgstr "**2. 性能表现**"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:71
msgid ""
"If the bonus token is accepted, the MTP model performs inference for "
"(num_speculative +1) tokens, including original main model output token "
"and bonus token. If rejected, inference is performed for less token, "
"determining on how many tokens accepted."
msgstr "如果 Bonus Token 被接受，MTP 模型将执行 (num_speculative + 1) 个 Token 的推理，包括主模型原始输出 Token 和 Bonus Token。如果被拒绝，则执行较少数量的 Token 推理，具体取决于有多少个 Token 被接受。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:73
msgid "DFX"
msgstr "DFX (设计可靠性)"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:75
msgid "Method Validation"
msgstr "方法验证"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:77
msgid ""
"Currently, the spec_decode scenario only supports methods such as ngram, "
"eagle, eagle3, and mtp. If an incorrect parameter is passed for the "
"method, the code will raise an error to alert the user that an incorrect "
"method was provided."
msgstr "目前，投机解码场景仅支持 ngram、eagle、eagle3 和 mtp 等方法。如果为 method 传递了错误的参数，代码将抛出错误，提示用户提供了不正确的方法。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:95
msgid "Integer Validation"
msgstr "整数验证"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:96
msgid ""
"The current npu_fused_infer_attention_score operator only supports "
"integers less than 16 per decode round. Therefore, the maximum supported "
"value for MTP is 15. If a value greater than 15 is provided, the code "
"will raise an error and alert the user."
msgstr "当前的 npu_fused_infer_attention_score 算子在每轮解码中仅支持小于 16 的整数。因此，MTP 支持的最大值为 15。如果提供的值大于 15，代码将报错并提醒用户。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:107
msgid "Limitation"
msgstr "局限性"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:108
msgid ""
"Due to the fact that only a single layer of weights is exposed in "
"DeepSeek's MTP, the accuracy and performance are not effectively "
"guaranteed in scenarios where MTP > 1 (especially MTP ≥ 3). Moreover, due"
" to current operator limitations, MTP supports a maximum of 15."
msgstr "由于 DeepSeek 的 MTP 仅公开了单层权重，在 MTP > 1（尤其是 MTP ≥ 3）的场景下，准确性和性能无法得到有效保证。此外，受限于当前算子，MTP 最大支持 15。"

#: ../../source/developer_guide/feature_guide/Multi_Token_Prediction.md:109
msgid ""
"In the fullgraph mode with MTP > 1, the capture size of each aclgraph "
"must be an integer multiple of (num_speculative_tokens + 1)."
msgstr "在 MTP > 1 的全图模式 (fullgraph mode) 下，每个 aclgraph 的捕获大小必须是 (num_speculative_tokens + 1) 的整数倍。"