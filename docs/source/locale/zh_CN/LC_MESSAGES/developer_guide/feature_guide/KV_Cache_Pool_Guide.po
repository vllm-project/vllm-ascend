# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:1
msgid "KV Cache Pool"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:3
msgid "Why KV Cache Pool?"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:5
msgid ""
"Prefix caching is an important feature in LLM inference that can reduce "
"prefill computation time drastically."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:7
msgid ""
"However, the performance gain from prefix caching is highly dependent on "
"cache hit rate, while cache hit rate can be limited if one only uses HBM "
"for kv cache storage."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:9
msgid ""
"Hence, KV Cache Pool is proposed to utilize various types of storages "
"including HBM,DRAM and SSD, making a pool for KV Cache storage, while "
"making the prefix of requests visible across all nodes, increasing the "
"cache hit rate for all requests."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:11
msgid ""
"vLLM Ascend currently supports [MooncakeStore](https://github.com"
"/kvcache-ai/Mooncake): one of the most recognized KV Cache storage "
"engine;"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:13
msgid ""
"While one can utilize mooncake store in vLLM V1 engine by setting it as a"
" remote backend of LMCache with GPU (see "
"[Tutorial](https://github.com/LMCache/LMCache/blob/dev/examples/kv_cache_reuse/remote_backends/mooncakestore/README.md)),"
" we find it would be better to integrate a connector that directly "
"supports mooncake store and can utilize the data transfer strategy to one"
" that is best fit to Huawei NPU hardware."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:15
msgid ""
"Hence, we propose to integrate Mooncake Store with a brand new "
"**MooncakeStoreConnectorV1**, which is indeed largly inspired by "
"**LMCacheConnectorV1** (see the `How is MooncakestoreConnectorV1 "
"Implemented?` section)."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:17
msgid "Usage"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:19
msgid ""
"vLLM Ascend Currently supports Mooncake Store for KV Cache Pool. To "
"enable Mooncake Store, one needs to config `kv-transfer-config` and "
"choose `MooncakeStoreConnector` as KV Connector."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:21
msgid ""
"For step-by-step deployment and configuration, please refer to the [KV "
"Pool User "
"Guide](https://docs.vllm.ai/projects/ascend/en/latest/user_guide/feature_guide/kv_pool.html)."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:23
msgid "How it works?"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:24
msgid ""
"The KV Cache Pool integrates multiple memory tiers (HBM, DRAM, SSD, etc.)"
" through a connector-based architecture."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:26
msgid ""
"Each connector implements a unified interface for storing, retrieving, "
"and transferring KV blocks between tiers, depending on access frequency "
"and hardware bandwidth."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:28
msgid ""
"When combined with vLLMâ€™s Prefix Caching mechanism, the pool enables "
"efficient caching both locally (in HBM) and globally (via Mooncake), "
"ensuring that frequently used prefixes remain hot while less frequently "
"accessed KV data can spill over to lower-cost memory."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:30
msgid "1. Combining KV Cache Pool with HBM Prefix Caching"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:31
msgid ""
"Prefix Caching with HBM is already supported by the vLLM V1 Engine. By "
"introducing KV Connector V1, users can seamlessly combine HBM-based "
"Prefix Caching with Mooncake-backed KV Pool."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:34
msgid ""
"The user can enable both features simply by enabling Prefix Caching, "
"which is enabled by default in vLLM V1 unless the "
"--no_enable_prefix_caching flag is set, and setting up the KV Connector "
"for KV Pool(e.g. the MooncakeStoreConnector)"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:36
msgid "**Workflow**:"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:38
msgid "The engine first checks for prefix hits in the HBM cache."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:40
msgid ""
"After getting the number of hit tokens on HBM, it queries the KV Pool via"
" the connector, if there is additional hits in KV Pool, we get the "
"**additional blocks only** from KV Pool, and get the rest of the blocks "
"directly from HBM to minimize the data transfer latency."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:42
msgid ""
"After the KV Caches in KV Pool is load into HBM, the remaining process is"
" the same as Prefix Caching in HBM."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:44
msgid "2. Combining KV Cache Pool with Mooncake PD Disaggregation"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:46
msgid ""
"When used together with Mooncake PD (Prefill-Decode) Disaggregation, the "
"KV Cache Pool can further decouple prefill and decode stages across "
"devices or nodes."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:48
msgid ""
"Currently, we only perform put and get operation of KV Pool for **Prefiil"
" Nodes**, and Decode Nodes get their KV Cache from Mooncake P2P KV "
"Connector, i.e. MooncakeConnector."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:50
msgid ""
"The key benefit of doing this is that we can keep the gain in performance"
" by computing less with Prefix Caching from HBM and KV Pool for Prefill "
"Nodes while not sacrificing the data transfer efficiency between Prefill "
"and Decode nodes with P2P KV Connector that transfer KV Caches between "
"NPU devices directly."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:52
msgid ""
"To Enable this feature, we need to setup both Mooncake Connector and "
"Mooncake Store connector with a Multi Connector, which is a KV Connector "
"class provided by vLLM that can call multiple KV Connectors in specific "
"order;"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:54
msgid ""
"For details, please also refer to the Mooncake Connector Store Deployment"
" Guide."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:56
msgid "How is MooncakestoreConnectorV1 Implemented?"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:57
msgid ""
"**MooncakestoreConnectorV1** inhereits the KV Connector V1 class in vLLM "
"V1: through implementing the required methods defined in the KV connector"
" V1 base class, one can integrate a thrid-party KV cache transfer/storage"
" backend into the vLLM framework."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:59
msgid ""
"MooncakeStoreConnectorV1 is also largly inspried by LMCacheConnectorV1 in"
" term of the `Lookup Engine`/`Lookup Client` design for looking up KV "
"cache keys, and the `ChunkedTokenDatabase` class for processing tokens "
"into prefix-aware hashes as well as other hashing related designs. On top"
" of this, we have also added our own design including `KVTransferThread` "
"that allows async `get` and `put` of KV caches with multi-threading, and "
"NPU-related data transfer optimization such as removing the `LocalBuffer`"
" in LMCache to remove redundant data transfer."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:61
msgid ""
"The KV Connector methods that need to be implemented can be categorized "
"into scheduler-side methods that are called in V1 scheduler and worker-"
"side methods that are called in V1 worker, namely:"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:62
msgid "KV Connector Scheduler-Side Methods:"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:63
msgid ""
"`get_num_new_matched_tokens`: Get prefix cache hit in number of tokens "
"through looking up into the KV pool.   `update_states_after_alloc`:  "
"Update KVConnector state after temporary buffer alloc.   "
"`build_connector_meta`: Attach the connector metadata to the request "
"object.   `request_finished`: Once a request is finished, determine "
"whether request blocks should be freed now or will be sent asynchronously"
" and freed later."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:67
msgid "Connector Worker-Side Methods:"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:68
msgid ""
"`register_kv_caches`: Register KV cache buffers needed for KV cache "
"transfer.   `start_load_kv`: Perform KV cache load operation that "
"transfers KV cache from storage to device.   `wait_for_layer_load`: "
"Optional; Wait for layer load in layerwise + async KV load scenario.   "
"`save_kv_layer`: Optional Do layerwise KV cache put into KV Pool.   "
"`wait_for_save`: Wait for KV Save to finish if async KV cache save/put."
"   `get_finished` Get request that finished KV transfer, `done_sending` "
"if `put` finished, `done_reciving` if `get` finished."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:75
msgid "DFX"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:76
msgid ""
"When looking up a key in KV Pool, if we cannot find the key, there is no "
"Cache Hit for this specific block; we return no hit for this block and do"
" not look up further blocks for current request."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:77
msgid ""
"Similaly, when we are trying to put a block into KV Pool and failed, we "
"do not put further blocks (subject to change)."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:79
msgid "Limitation"
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:81
msgid ""
"Currently, Mooncake Store for vLLM-Ascend only supports DRAM as the "
"storage for KV Cache pool."
msgstr ""

#: ../../source/developer_guide/feature_guide/KV_Cache_Pool_Guide.md:83
msgid ""
"For now, if we successfully looked up a key and found it exists, but "
"failed to get it when calling KV Pool's get function, we just output a "
"log indicating the get operation failed and keep going; hence, the "
"accuracy of that specific request may be affected. We will handle this "
"situation by falling back the request and re-compute everything assuming "
"there's no prefix cache hit (or even better, revert only one block and "
"keep using the Prefix Caches before that)."
msgstr ""

