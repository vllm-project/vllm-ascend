# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:1
msgid "Multi Token Prediction (MTP)"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:3
msgid "Why We Need MTP"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:4
msgid ""
"MTP boosts inference performance by parallelizing the prediction of "
"multiple tokens, shifting from single-token to multi-token generation. "
"This approach significantly increases generation throughput and achieves "
"multiplicative acceleration in inference speed—all without compromising "
"output quality."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:6
msgid "How to Use MTP"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:7
msgid ""
"To enable MTP for DeepSeek-V3 models, add the following parameter when "
"starting the service:"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:9
#, python-brace-format
msgid ""
"--speculative_config ' {\"method\": \"mtp\", \"num_speculative_tokens\": "
"1, \"disable_padded_drafter_batch\": False} '"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:11
msgid ""
"`num_speculative_tokens`: The number of speculative tokens which enable "
"model to predict multiple tokens at once, if provided. It will default to"
" the number in the draft model config if present, otherwise, it is "
"required."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:12
msgid ""
"`disable_padded_drafter_batch`: Disable input padding for speculative "
"decoding. If set to True, speculative input batches can contain sequences"
" of different lengths, which may only be supported by certain attention "
"backends. This currently only affects the MTP method of speculation, "
"default is False."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:14
msgid "How It Works"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:16
msgid "Module Architecture"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:27
msgid "**1. sample**"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:29
msgid ""
"*rejection_sample.py*: During decoding, the main model processes the "
"previous round’s output token and the predicted token together (computing"
" 1+k tokens simultaneously). The first token is always correct, while the"
" second token—referred to as the **bonus token**—is uncertain since it is"
" derived from speculative prediction, thus We employ **Greedy Strategy** "
"and **Rejection Sampling Strategy** to determine whether the bonus token "
"should be accepted. The module structure consists of an "
"`AscendRejectionSampler` class with a forward method that implements the "
"specific sampling logic."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:37
msgid "**2. spec_decode**"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:39
msgid ""
"This section encompasses the model preprocessing for spec-decode, "
"primarily structured as follows: it includes loading the model, executing"
" a dummy run, and generating token ids. These steps collectively form the"
" model data construction and forward invocation for a single spec-decode "
"operation."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:40
msgid ""
"*mtp_proposer.py*: Configure vLLM-Ascend to use speculative decoding "
"where proposals are generated by deepseek mtp layer."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:52
msgid "Algorithm"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:54
msgid "**1. Reject_Sample**"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:55
msgid "*Greedy Strategy*"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:57
msgid ""
"Verify whether the token generated by the main model matches the "
"speculative token predicted by MTP in the previous round. If they match "
"exactly, accept the bonus token; otherwise, reject it and any subsequent "
"tokens derived from that speculation."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:59
msgid "*Rejection Sampling Strategy*"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:61
msgid "This method introduces stochasticity in rejection sampling."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:63
msgid ""
"For each draft token, acceptance is determined by verifying whether the "
"inequality `P_target / P_draft ≥ U` holds, where `P_target` represents "
"the probability assigned to the current draft token by the target model, "
"`P_draft` denotes the probability assigned by the draft model, and `U` is"
" a random number sampled uniformly from the interval [0, 1)."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:65
msgid ""
"The decision logic for each draft token is as follows: if the inequality "
"`P_target / P_draft ≥ U` holds, the draft token is accepted as output; "
"conversely, if `P_target / P_draft < U`, the draft token is rejected."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:67
msgid ""
"When a draft token is rejected, a recovery sampling process is triggered "
"where a \"recovered token\" is resampled from the adjusted probability "
"distribution defined as `Q = max(P_target - P_draft, 0)`. In the current "
"MTP implementation, since `P_draft` is not provided and defaults to 1, "
"the formulas simplify such that token acceptance occurs when `P_target ≥ "
"U,` and the recovery distribution becomes `Q = max(P_target - 1, 0)`."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:69
msgid "**2. Performance**"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:71
msgid ""
"If the bonus token is accepted, the MTP model performs inference for "
"(num_speculative +1) tokens, including original main model output token "
"and bonus token. If rejected, inference is performed for less token, "
"determining on how many tokens accepted."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:73
msgid "DFX"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:75
msgid "Method Validation"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:77
msgid ""
"Currently, the spec_decode scenario only supports methods such as ngram, "
"eagle, eagle3, and mtp. If an incorrect parameter is passed for the "
"method, the code will raise an error to alert the user that an incorrect "
"method was provided."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:95
msgid "Integer Validation"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:96
msgid ""
"The current npu_fused_infer_attention_score operator only supports "
"integers less than 16 per decode round. Therefore, the maximum supported "
"value for MTP is 15. If a value greater than 15 is provided, the code "
"will raise an error and alert the user."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:107
msgid "Limitation"
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:108
msgid ""
"Due to the fact that only a single layer of weights is exposed in "
"DeepSeek's MTP, the accuracy and performance are not effectively "
"guaranteed in scenarios where MTP > 1 (especially MTP ≥ 3). Moreover, due"
" to current operator limitations, MTP supports a maximum of 15."
msgstr ""

#: ../../source/user_guide/feature_guide/Multi_Token_Prediction.md:109
msgid ""
"In the fullgraph mode with MTP > 1, the capture size of each aclgraph "
"must be an integer multiple of (num_speculative_tokens + 1)."
msgstr ""

