# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/user_guide/feature_guide/large_scale_ep.md:1
msgid "Distributed DP Server With Large Scale Expert Parallelism"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:3
msgid "Getting Start"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:5
msgid ""
"vLLM-Ascend now supports prefill-decode (PD) disaggregation in the large "
"scale **Expert  Parallelism (EP)** scenario. To achieve better "
"performance，the distributed DP server is applied in vLLM-Ascend. In the "
"PD separation scenario, different optimization strategies can be "
"implemented based on the distinct characteristics of PD nodes, thereby "
"enabling more flexible model deployment. \\ Take the deepseek model as an"
" example, use 8 Atlas 800T A3 servers to deploy the model. Assume the ip "
"of the servers start from 192.0.0.1, and end by 192.0.0.8. Use the first "
"4 servers as prefiller nodes and the last 4 servers as decoder nodes. And"
" the prefiller nodes deployed as master node independently, the decoder "
"nodes set 192.0.0.5 node to be the master node."
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:8
msgid "Verify Multi-Node Communication Environment"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:10
msgid "Physical Layer Requirements:"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:12
msgid ""
"The physical machines must be located on the same WLAN, with network "
"connectivity."
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:13
msgid ""
"All NPUs must be interconnected. For the Atlas A2 generation, intra-node "
"connectivity is via HCCS, and inter-node connectivity is via RDMA. For "
"the Atlas A3 generation, both intra-node and inter-node connectivity are "
"via HCCS."
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:15
msgid "Verification Process:"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md
msgid "A3"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:22
#: ../../source/user_guide/feature_guide/large_scale_ep.md:64
msgid "Single Node Verification:"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:24
#: ../../source/user_guide/feature_guide/large_scale_ep.md:66
msgid ""
"Execute the following commands on each node in sequence. The results must"
" all be `success` and the status must be `UP`:"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:41
#: ../../source/user_guide/feature_guide/large_scale_ep.md:83
msgid "Get NPU IP Addresses"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:47
msgid "Get superpodid and SDID"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:53
#: ../../source/user_guide/feature_guide/large_scale_ep.md:89
msgid "Cross-Node PING Test"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md
msgid "A2"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:98
msgid "Large Scale EP model deployment"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:100
msgid "Generate script with configurations"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:102
msgid ""
"In the PD separation scenario, we provide a optimized configuration. You "
"can use the following shell script for configuring the prefiller and "
"decoder nodes respectively."
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md
msgid "Prefiller node"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md
msgid "Decoder node"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:243
msgid "Start Distributed DP Server for prefill-decode disaggregation"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:245
msgid ""
"Execute the following Python file on all nodes to use the distributed DP "
"server. (We recommend using this feature on the v0.9.1 official release)"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:319
msgid ""
"Note that the prefiller nodes and the decoder nodes may have different "
"configurations. In this example, each prefiller node deployed as master "
"node independently, but all decoder nodes take the first node as the "
"master node. So it leads to difference in 'dp_size_local' and "
"'dp_rank_start'"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:321
msgid "Example proxy for Distributed DP Server"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:323
msgid ""
"In the PD separation scenario, we need a proxy to distribute requests. "
"Execute the following commands to enable the example proxy:"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Parameter"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "meaning"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "--port"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Proxy service Port"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "--host"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Proxy service Host IP"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "--prefiller-hosts"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Hosts of prefiller nodes"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "--prefiller-hosts-num"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Number of repetitions for prefiller node hosts"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "--prefiller-ports"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Ports of prefiller nodes"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "--prefiller-ports-inc"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Number of increments for prefiller node ports"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "--decoder-hosts"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Hosts of decoder nodes"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "--decoder-hosts-num"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Number of repetitions for decoder node hosts"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "--decoder-ports"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Ports of decoder nodes"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "--decoder-ports-inc"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "Number of increments for decoder node ports"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:366
msgid ""
"You can get the proxy program in the repository's examples, "
"[load\\_balance\\_proxy\\_server\\_example.py](https://github.com/vllm-"
"project/vllm-"
"ascend/blob/v0.9.1-dev/examples/disaggregate_prefill_v1/load_balance_proxy_server_example.py)"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:368
msgid "Benchmark"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:370
msgid ""
"We recommend use aisbench tool to assess performance. "
"[aisbench](https://gitee.com/aisbench/benchmark) Execute the following "
"commands to install aisbench"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:378
msgid ""
"You need to canncel the http proxy before assessing performance, as "
"following"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:386
msgid "You can place your datasets in the dir: `benchmark/ais_bench/datasets`"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:387
msgid ""
"You can change the configurationin the dir "
":`benchmark/ais_bench/benchmark/configs/models/vllm_api` Take the "
"``vllm_api_stream_chat.py`` for examples"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:413
msgid ""
"Take gsm8k dataset for example, execute the following commands  to assess"
" performance."
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:419
msgid ""
"For more details for commands and parameters for aisbench, refer to  "
"[aisbench](https://gitee.com/aisbench/benchmark)"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:421
msgid "Prefill & Decode Configuration Details"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:423
msgid "In the PD separation scenario, we provide a optimized configuration."
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:425
msgid "**prefiller node**"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:427
msgid "set HCCL_BUFFSIZE=256"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:428
msgid "add '--enforce-eager' command to 'vllm serve'"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:429
#: ../../source/user_guide/feature_guide/large_scale_ep.md:452
msgid "Take '--kv-transfer-config' as follow"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:443
#: ../../source/user_guide/feature_guide/large_scale_ep.md:466
msgid "Take '--additional-config' as follow"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:449
msgid "**decoder node**"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:451
msgid "set HCCL_BUFFSIZE=1024"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:472
msgid "Parameters Description"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:474
msgid "1.'--additional-config'  Parameter Introduction:"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:476
msgid ""
"**\"enable_weight_nz_layout\"：** Whether to convert quantized weights to "
"NZ format to accelerate matrix multiplication."
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:477
msgid ""
"**\"enable_prefill_optimizations\"：** Whether to enable DeepSeek models' "
"prefill optimizations. <br>"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:480
msgid "3.enable MTP Add the following command to your configurations."
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:487
msgid "Recommended Configuration Example"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:489
msgid ""
"For example，if the average input length is 3.5k, and the output length is"
" 1.1k, the context length is 16k, the max length of the input dataset is "
"7K. In this scenario, we give a recommended configuration for distributed"
" DP server with high EP. Here we use 4 nodes for prefill and 4 nodes for "
"decode."
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "node"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "DP"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "TP"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "EP"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "max-model-len"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "max-num-batched-tokens"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "max-num-seqs"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "gpu-memory-utilization"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "prefill"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "2"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "8"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "16"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "17000"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "16384"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "4"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "0.9"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "decode"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "64"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "1"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "256"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:284
msgid "28"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:497
msgid ""
"Note that these configurations are not related to optimization. You need "
"to adjust these parameters based on actual scenarios."
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:500
msgid "FAQ"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:502
msgid "1. Prefiller nodes need to warmup"
msgstr ""

#: ../../source/user_guide/feature_guide/large_scale_ep.md:504
msgid ""
"Since the computation of some NPU operators requires several rounds of "
"warm-up to achieve best performance, we recommend preheating the service "
"with some requests before conducting performance tests to achieve the "
"best end-to-end throughput."
msgstr ""

