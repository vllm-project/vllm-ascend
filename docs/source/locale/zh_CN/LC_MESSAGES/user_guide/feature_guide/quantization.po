# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#

msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-17 12:01+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/user_guide/feature_guide/quantization.md:1
msgid "Quantization Guide"
msgstr "量化指南"

#: ../../source/user_guide/feature_guide/quantization.md:3
msgid "Model quantization is a technique that reduces the size and computational requirements of a model by lowering the data precision of the weights and activation values in the model, thereby saving the memory and improving the inference speed."
msgstr "模型量化是一种通过降低模型中权重和激活值的数据精度，从而减少模型大小和计算需求的技术，这样可以节省内存并提高推理速度。"

#: ../../source/user_guide/feature_guide/quantization.md:5
msgid "Since version 0.9.0rc2, the quantization feature is experimentally supported by vLLM Ascend. Users can enable the quantization feature by specifying `--quantization ascend`. Currently, only Qwen, DeepSeek series models are well tested. We will support more quantization algorithms and models in the future."
msgstr "自 0.9.0rc2 版本起，vLLM Ascend 实验性地支持量化特性。用户可以通过指定 `--quantization ascend` 启用量化功能。目前，只有 Qwen、DeepSeek 系列模型经过充分测试。未来我们将支持更多量化算法和模型。"

#: ../../source/user_guide/feature_guide/quantization.md:7
msgid "Install ModelSlim"
msgstr "安装 ModelSlim"

#: ../../source/user_guide/feature_guide/quantization.md:9
msgid "To quantize a model, you should install [ModelSlim](https://gitee.com/ascend/msit/blob/master/msmodelslim/README.md) which is the Ascend compression and acceleration tool. It is an affinity-based compression tool designed for acceleration, using compression as its core technology and built upon the Ascend platform."
msgstr "要对模型进行量化，用户应安装 [ModelSlim](https://gitee.com/ascend/msit/blob/master/msmodelslim/README.md)，这是昇腾的压缩与加速工具。它是一种基于亲和性的压缩工具，专为加速设计，以压缩为核心技术，并基于昇腾平台构建。"

#: ../../source/user_guide/feature_guide/quantization.md:11
msgid "Install ModelSlim:"
msgstr "安装 ModelSlim："

#: ../../source/user_guide/feature_guide/quantization.md:23
msgid "Quantize model"
msgstr "量化模型"

#: ../../source/user_guide/feature_guide/quantization.md:26
msgid "You can choose to convert the model yourself or use the quantized model we uploaded. See https://www.modelscope.cn/models/vllm-ascend/Kimi-K2-Instruct-W8A8. This conversion process requires a larger CPU memory, ensure that the RAM size is greater than 2 TB."
msgstr "你可以选择自行转换模型，或使用我们上传的量化模型，详见 https://www.modelscope.cn/models/vllm-ascend/Kimi-K2-Instruct-W8A8。该转换过程需要较大的 CPU 内存，请确保 RAM 大小超过 2 TB。"

#: ../../source/user_guide/feature_guide/quantization.md:31
msgid "Adapt to changes"
msgstr "适应变化"

#: ../../source/user_guide/feature_guide/quantization.md:32
msgid "Ascend does not support the `flash_attn` library. To run the model, you need to follow the [guide](https://gitee.com/ascend/msit/blob/master/msmodelslim/example/DeepSeek/README.md#deepseek-v3r1) and comment out certain parts of the code in `modeling_deepseek.py` located in the weights folder."
msgstr "昇腾不支持 `flash_attn` 库。要运行模型，你需要按照 [指南](https://gitee.com/ascend/msit/blob/master/msmodelslim/example/DeepSeek/README.md#deepseek-v3r1) 注释掉权重文件夹中 `modeling_deepseek.py` 的部分代码。"

#: ../../source/user_guide/feature_guide/quantization.md:33
msgid "The current version of transformers does not support loading weights in FP8 quantization format. you need to follow the [guide](https://gitee.com/ascend/msit/blob/master/msmodelslim/example/DeepSeek/README.md#deepseek-v3r1) and delete the quantization related fields from `config.json` in the weights folder."
msgstr "当前版本的 transformers 不支持加载 FP8 量化格式的权重，你需要按照 [指南](https://gitee.com/ascend/msit/blob/master/msmodelslim/example/DeepSeek/README.md#deepseek-v3r1) 删除权重文件夹中 `config.json` 中的量化相关字段。"

#: ../../source/user_guide/feature_guide/quantization.md:35
msgid "Generate the W8A8 weights"
msgstr "生成 W8A8 权重"

#: ../../source/user_guide/feature_guide/quantization.md:48
msgid "Here is the full converted model files except safetensors:"
msgstr "以下是完整转换后的模型文件（不含 safetensors）："

#: ../../source/user_guide/feature_guide/quantization.md:64
msgid "Run the model"
msgstr "运行模型"

#: ../../source/user_guide/feature_guide/quantization.md:66
msgid "Now, you can run the quantized model with vLLM Ascend. Examples for online and offline inference are provided as follows:"
msgstr "现在，你可以使用 vLLM Ascend 运行量化模型。下面提供在线和离线推理示例："

#: ../../source/user_guide/feature_guide/quantization.md:68
msgid "Offline inference"
msgstr "离线推理"

#: ../../source/user_guide/feature_guide/quantization.md:94
msgid "Online inference"
msgstr "在线推理"

#: ../../source/user_guide/feature_guide/quantization.md:96
msgid "Enable quantization by specifying `--quantization ascend`, for more details, see the [DeepSeek-V3-W8A8 Tutorial](https://vllm-ascend.readthedocs.io/en/latest/tutorials/multi_node.html)."
msgstr "通过指定 `--quantization ascend` 来启用量化，更多详情请参见 [DeepSeek-V3-W8A8 教程](https://vllm-ascend.readthedocs.io/en/latest/tutorials/multi_node.html)。"

#: ../../source/user_guide/feature_guide/quantization.md:98
msgid "FAQs"
msgstr "常见问题解答"

#: ../../source/user_guide/feature_guide/quantization.md:100
msgid "1. How to solve the KeyError \"xxx.layers.0.self_attn.q_proj.weight\"?"
msgstr "1. 如何解决 KeyError: 'xxx.layers.0.self_attn.q_proj.weight' 问题？"

#: ../../source/user_guide/feature_guide/quantization.md:102
msgid "First, make sure you specify `ascend` as the quantization method. Second, check if your model is converted by the `br_release_MindStudio_8.1.RC2_TR5_20260624` ModelSlim version. Finally, if it still does not work, submit an issue. Maybe some new models need to be adapted."
msgstr "首先，请确保你指定了 `ascend` 量化方法。其次，检查你的模型是否由 `br_release_MindStudio_8.1.RC2_TR5_20260624` ModelSlim 版本转换。最后，如果仍然无法使用，请提交 issue，可能有一些新模型需要适配。"

#: ../../source/user_guide/feature_guide/quantization.md:104
msgid "2. How to solve the error \"Could not locate the configuration_deepseek.py\"?"
msgstr "2. 如何解决“无法找到 configuration_deepseek.py”错误？"

#: ../../source/user_guide/feature_guide/quantization.md:106
msgid "Please convert DeepSeek series models using `br_release_MindStudio_8.1.RC2_TR5_20260624` ModelSlim, where the missing configuration_deepseek.py error has been fixed."
msgstr "请使用 `br_release_MindStudio_8.1.RC2_TR5_20260624` ModelSlim 转换 DeepSeek 系列模型，该版本已修复缺少 configuration_deepseek.py 的错误。"

#: ../../source/user_guide/feature_guide/quantization.md:108
msgid "3. What should be considered when converting DeepSeek series models with ModelSlim?"
msgstr "3. 使用 ModelSlim 转换 DeepSeek 系列模型时应注意什么？"

#: ../../source/user_guide/feature_guide/quantization.md:110
msgid "When the MLA portion of the weights used the `W8A8_DYNAMIC` quantization with the torchair graph mode enabled, modify the configuration file in the CANN package to prevent incorrect inference results."
msgstr "当权重的 MLA 部分使用 `W8A8_DYNAMIC` 量化且启用了 torchair 图模式时，请修改 CANN 包中的配置文件以防止推理结果错误。"

#: ../../source/user_guide/feature_guide/quantization.md:112
msgid "The operation steps are as follows:"
msgstr "操作步骤如下："

#: ../../source/user_guide/feature_guide/quantization.md:114
msgid "Search in the CANN package directory, for example: find /usr/local/Ascend/ -name fusion_config.json"
msgstr "在 CANN 包目录中搜索，例如：find /usr/local/Ascend/ -name fusion_config.json"

#: ../../source/user_guide/feature_guide/quantization.md:117
msgid "Add `\"AddRmsNormDynamicQuantFusionPass\":\"off\",` and `\"MultiAddRmsNormDynamicQuantFusionPass\":\"off\",` to the fusion_config.json you find, the location is as follows:"
msgstr "将 `\"AddRmsNormDynamicQuantFusionPass\":\"off\",` 和 `\"MultiAddRmsNormDynamicQuantFusionPass\":\"off\",` 添加到你找到的 fusion_config.json 文件中，位置如下："
