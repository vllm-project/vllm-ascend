# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/user_guide/feature_guide/ucm_deployment.md:1
msgid "UCM-Enhanced Prefix Caching Deployment Guide"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:3
msgid "Overview"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:5
msgid ""
"Unified Cache Management (UCM) provides an external KV-cache storage "
"layer designed for prefix-caching scenarios in vLLM/vLLM-Ascend. Unlike "
"KV Pooling, which expands prefix-cache capacity only by aggregating "
"device memory and therefore remains limited by HBM/DRAM size and lacks "
"persistence, UCM decouples compute from storage and adopts a tiered "
"design. Each node uses local DRAM as a fast cache, while a shared "
"backend—such as 3FS or enterprise-grade storage—serves as the persistent "
"KV store. This approach removes the capacity ceiling imposed by device "
"memory, enables durable and reliable prefix caching, and allows cache "
"capacity to scale with the storage system rather than with compute "
"resources."
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:7
msgid "Prerequisites"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:9
msgid "OS: Linux"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:10
msgid "A hardware with Ascend NPU. It’s usually the Atlas 800 A2 series."
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:11
msgid "**vLLM: main branch**"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:12
msgid "**vLLM Ascend: main branch**"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:14
msgid "UCM Installation"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:16
msgid ""
"**Please refer to the [official UCM installation guide for Ascend "
"NPU](https://ucm.readthedocs.io/en/latest/getting-"
"started/quickstart_vllm_ascend.html)**"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:18
msgid "Configure UCM for Prefix Caching"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:20
msgid ""
"Modify the UCM configuration file to specify which UCM connector to use "
"and where KV blocks should be stored.   You may directly edit the example"
" file at:"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:23
msgid "`unified-cache-management/examples/ucm_config_example.yaml`"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:25
msgid ""
"**For updated configuration options, please refer to the [official UCM "
"documentation for prefix-caching](https://ucm.readthedocs.io/en/latest"
"/user-guide/prefix-cache/nfs_store.html)**"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:27
msgid "A minimal configuration looks like this:"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:39
msgid "Explanation:"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:41
msgid ""
"ucm_connector_name: \"UcmNfsStore\": Specifies `UcmNfsStore` as the UCM "
"connector."
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:44
msgid ""
"storage_backends: Specify the directory used for storing KV blocks. It "
"can be a local directory or an NFS-mounted path. UCM will store KV blocks"
" here.  **⚠️ Make sure to replace `\"/mnt/test\"` with your actual "
"storage directory.**"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:48
msgid "use_direct: Whether to enable direct I/O (optional). Default is `false`."
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:51
msgid ""
"load_only_first_rank: Controls whether only rank 0 loads KV cache and "
"broadcasts it to other ranks.   This feature is currently not supported "
"on Ascend, so it must be set to `false` (all ranks load/dump "
"independently)."
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:55
msgid "Launching Inference"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:57
msgid ""
"In this guide, we describe **online inference** using vLLM with the UCM "
"connector, deployed as an OpenAI-compatible server. For best performance "
"with UCM, it is recommended to set `block_size` to 128."
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:59
msgid "To start the vLLM server with the Qwen/Qwen2.5-14B-Instruct model, run:"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:79
msgid ""
"**⚠️ Make sure to replace `\"/vllm-workspace/unified-cache-"
"management/examples/ucm_config_example.yaml\"` with your actual config "
"file path.**"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:81
msgid "If you see log as below:"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:89
msgid ""
"Congratulations, you have successfully started the vLLM server with UCM "
"connector!"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:91
msgid "Evaluating UCM Prefix Caching Performance"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:92
msgid ""
"After launching the vLLM server with `UCMConnector` enabled, the easiest "
"way to observe the prefix caching effect is to run the built-in `vllm "
"bench` CLI. Executing the following command **twice** in a separate "
"terminal shows the improvement clearly."
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:111
msgid "After the first execution"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:112
msgid "The `vllm bench` terminal prints the benchmark result:"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:119
msgid "Inspecting the vLLM server logs reveals entries like:"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:125
msgid ""
"This indicates that for the first inference request, UCM did not hit any "
"cached KV blocks. As a result, the full 16K-token prefill must be "
"computed, leading to a relatively large TTFT."
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:127
msgid "After the second execution"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:128
msgid "Running the same benchmark again produces:"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:135
msgid "The vLLM server logs now contain similar entries:"
msgstr ""

#: ../../source/user_guide/feature_guide/ucm_deployment.md:141
msgid ""
"This indicates that during the second request, UCM successfully retrieved"
" all 125 cached KV blocks from the storage backend. Leveraging the fully "
"cached prefix significantly reduces the initial latency observed by the "
"model, yielding an approximate **8× improvement in TTFT** compared to the"
" initial run."
msgstr ""

