# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:1
msgid "Speculative Decoding Guide"
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:3
msgid ""
"This guide shows how to use Speculative Decoding with vLLM Ascend. "
"Speculative decoding is a technique which improves inter-token latency in"
" memory-bound LLM inference."
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:5
msgid "Speculating by matching n-grams in the prompt"
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:6
msgid ""
"The following code configures vLLM Ascend to use speculative decoding "
"where proposals are generated by matching n-grams in the prompt."
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:8
#: ../../source/user_guide/feature_guide/speculative_decoding.md:41
#: ../../source/user_guide/feature_guide/speculative_decoding.md:121
msgid "Offline inference"
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:35
msgid "Speculating using EAGLE based draft models"
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:37
msgid ""
"The following code configures vLLM Ascend to use speculative decoding "
"where proposals are generated by an [EAGLE (Extrapolation Algorithm for "
"Greater Language-model Efficiency)](https://arxiv.org/pdf/2401.15077) "
"based draft model."
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:39
msgid ""
"In v0.12.0rc1 of vLLM Ascend, the async scheduler is more stable and "
"ready to be enabled. We have adapted it to support EAGLE, and you can use"
" it by setting `async_scheduling=True` as follows. If you encounter any "
"issues, please feel free to open an issue on GitHub. As a workaround, you"
" can disable this feature by unsetting `async_scheduling=True` when "
"initializing the model."
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:73
msgid ""
"A few important things to consider when using the EAGLE based draft "
"models:"
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:75
msgid ""
"The EAGLE draft models available in the [HF repository for EAGLE "
"models](https://huggingface.co/yuhuili) should be loaded and used "
"directly by vLLM. This functionality was added in PR "
"[#4893](https://github.com/vllm-project/vllm-ascend/pull/4893). If you "
"are using a vLLM version released before this pull request was merged, "
"please update to a more recent version."
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:79
msgid ""
"The EAGLE based draft models need to be run without tensor parallelism "
"(i.e. draft_tensor_parallel_size is set to 1 in `speculative_config`), "
"although it is possible to run the main model using tensor parallelism "
"(see example above)."
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:83
msgid ""
"When using EAGLE-3 based draft model, option \"method\" must be set to "
"\"eagle3\". That is, to specify `\"method\": \"eagle3\"` in "
"`speculative_config`."
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:86
msgid "Speculating using MTP speculators"
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:88
msgid ""
"The following code configures vLLM Ascend to use speculative decoding "
"where proposals are generated by MTP (Multi Token Prediction), boosting "
"inference performance by parallelizing the prediction of multiple tokens."
" For more information about MTP see "
"[Multi_Token_Prediction](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/feature_guide/Multi_Token_Prediction.html)"
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:90
msgid "Online inference"
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:110
msgid "Speculating using Suffix Decoding"
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:112
msgid ""
"The following code configures vLLM to use speculative decoding where "
"proposals are generated using Suffix Decoding [(SuffixDecoding: Extreme "
"Speculative Decoding for Emerging AI "
"Applications)](https://arxiv.org/abs/2411.04975)."
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:114
msgid ""
"Like n-gram, Suffix Decoding can generate draft tokens by pattern-"
"matching using the last `n` generated tokens. Unlike n-gram, Suffix "
"Decoding (1) can pattern-match against both the prompt and previous "
"generations, (2) uses frequency counts to propose the most likely "
"continuations, and (3) speculates an adaptive number of tokens for each "
"request at each iteration to get better acceptance rates."
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:116
msgid ""
"Suffix Decoding can achieve better performance for tasks with high "
"repetition, such as code-editing, agentic loops (e.g. self-reflection, "
"self-consistency), and RL rollouts."
msgstr ""

#: ../../source/user_guide/feature_guide/speculative_decoding.md:118
msgid ""
"[!NOTE] Suffix Decoding requires Arctic Inference. You can install it "
"with `pip install arctic-inference`."
msgstr ""

