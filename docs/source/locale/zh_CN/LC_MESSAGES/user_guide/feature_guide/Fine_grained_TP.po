# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:1
msgid "Fine-Grained Tensor Parallelism (Finegrained TP)"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:3
msgid "Overview"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:5
msgid ""
"Fine-Grained Tensor Parallelism (Finegrained TP) extends standard tensor "
"parallelism by enabling **independent tensor parallel sizes for different"
" model components**. Instead of applying a single global "
"`tensor_parallel_size` to all layers, Finegrained TP allows users to "
"configure separate TP size for key modules—such as embedding, language "
"model head (lm_head), attention output projection (oproj), and MLP "
"blocks—via the `finegrained_tp_config` parameter."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:7
msgid ""
"This capability supports heterogeneous parallelism strategies within a "
"single model, providing finer control over weight distribution, memory "
"layout, and communication patterns across devices. The feature is "
"compatible with standard dense transformer architectures and integrates "
"seamlessly into vLLM’s serving pipeline."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:11
msgid "Benefits of Finegrained TP"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:13
msgid ""
"Fine-Grained Tensor Parallelism delivers two primary performance "
"advantages through targeted weight sharding:"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:15
msgid ""
"**Reduced Per-Device Memory Footprint**：   Finegrained TP shards large "
"weight matrices（如 LM Head、o_proj）across devices, lowering peak memory "
"usage and enabling larger batches or deployment on memory-limited "
"hardware—without quantization."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:18
msgid ""
"**Faster Memory Access in GEMMs**：   In decode-heavy workloads, GEMM "
"performance is often memory-bound. Weight sharding reduces per-device "
"weight fetch volume, cutting DRAM traffic and improving bandwidth "
"efficiency—especially for latency-sensitive layers like LM Head and "
"o_proj."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:21
msgid ""
"Together, these effects allow practitioners to better balance memory, "
"communication, and compute—particularly in high-concurrency serving "
"scenarios—while maintaining compatibility with standard dense transformer"
" models."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:25
msgid "Supported Scenarios"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:27
msgid "Models"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:28
msgid ""
"Finegrained TP is **model-agnostic** and supports all standard dense "
"transformer architectures, including Llama, Qwen, DeepSeek (base/dense "
"variants), and others."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:30
msgid "Component & Execution Mode Support"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "TP config"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "Eager"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "Graph"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "Hybrid"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "Prefill"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "Decode"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "**embedding**"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "✅"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "**o_proj**"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "❌"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "**mlp**"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "**LMhead**"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:39
msgid "⚠️ Note:"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:40
msgid ""
"`o_proj` TP is only supported in Graph mode during Decode, because "
"dummy_run in eager mode will not trigger o_proj."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:41
msgid ""
"`mlp` TP supports dense models, or dense layers in MoE models. For "
"example, the first three dense layers of DeepSeek-R1."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:43
msgid "Configuration Limit:"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:44
msgid "The Fine-Grained TP size for any component must:"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:45
msgid "Be **≤ the data-parallel (DP) size**, and"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:46
msgid ""
"**Evenly divide the DP size** (i.e., `dp_size % tp_size == 0`) to ensure "
"valid device assignment and communication grouping."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:47
msgid ""
"⚠️ Violating these constraints will result in runtime errors or undefined"
" behavior."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:51
msgid "How to Use Finegrained TP"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:53
msgid "Configuration Format:"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:55
msgid ""
"Finegrained TP is controlled via the `finegrained_tp_config` field inside"
" `--additional-config`."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:68
msgid "Example Usage:"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:86
msgid "Experimental Results"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:88
msgid ""
"To evaluate the effectiveness of fine-grained TP in large-scale service "
"scenarios, we use the model **DeepSeek-R1-W8A8**, deploy PD-separated "
"decode instances in the environment of 32 cards Ascend 910B*64G (A2), "
"with parallel configuration as DP32+EP32, and fine-grained TP size of 8, "
"the performance data is as follows."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "Module"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "Memory Savings"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "TPOT Impact (batch=24)"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "o_proj TP = 8"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "5.8 GB"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "**+1.5 ms** (degradation)"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "LM head TP = 8"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "1.51 GB"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "**−1.2 ms** (improvement)"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "FFN TP = 8"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "0.9 GB"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "**−1.0 ms** (improvement)"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "Embedding TP = 8"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "**Total**"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "**9.72 GB**"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md
msgid "—"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:98
msgid ""
"We achieved significant gains in terms of high memory capacity on a "
"single card, as well as the benefits of TPOT."
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:101
msgid "✅ Deployment Recommendations"
msgstr ""

#: ../../source/user_guide/feature_guide/Fine_grained_TP.md:103
msgid ""
"Fine-grained TP is the **most effective** in the **decode instance** of "
"PD separation, where models are typically deployed in all-DP mode. In "
"this setup, sharding weight-heavy layers reduces redundant storage and "
"memory pressure."
msgstr ""

