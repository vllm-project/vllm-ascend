# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/tutorials/Qwen3-32B-W4A4.md:1
msgid "Qwen3-32B-W4A4"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:3
msgid "Introduction"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:5
msgid ""
"W4A4 Flat Quantization is for better model compression and inference "
"efficiency on Ascend devices. And W4A4 is supported since `v0.11.0rc1`. "
"For modelslim, W4A4 is supported since `tag_MindStudio_8.2.RC1.B120_002`."
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:8
msgid "The following steps will show how to quantize Qwen3 32B to W4A4."
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:10
msgid "Environment Preparation"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:12
msgid "Run Docker Container"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:35
msgid "Install modelslim and Convert Model"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:38
msgid ""
"You can choose to convert the model yourself or use the quantized model "
"we uploaded, see https://www.modelscope.cn/models/vllm-ascend/Qwen3-32B-"
"W4A4"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:68
msgid "Verify the Quantized Model"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:70
msgid "The converted model files look like:"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:95
msgid "Deployment"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:97
msgid "Online Serving on Single NPU"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:103
msgid "Once your server is started, you can query the model with input prompts."
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:118
msgid "Offline Inference on Single NPU"
msgstr ""

#: ../../source/tutorials/Qwen3-32B-W4A4.md:121
msgid "To enable quantization for ascend, quantization method must be \"ascend\"."
msgstr ""

