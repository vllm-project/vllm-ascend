# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:1
msgid "Qwen3-Omni-30B-A3B-Thinking"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:3
msgid "Introduction"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:5
msgid ""
"Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation "
"models. It processes text, images, audio, and video, and delivers real-"
"time streaming responses in both text and natural speech. We introduce "
"several architectural upgrades to improve performance and efficiency. The"
" Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, "
"equipped with chain-of-thought reasoning, supporting audio, video, and "
"text input, with text output."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:7
msgid ""
"This document will show the main verification steps of the model, "
"including supported features, feature configuration, environment "
"preparation, single-node deployment, accuracy and performance evaluation."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:9
msgid "Supported Features"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:10
msgid ""
"Refer to [supported features](https://docs.vllm.ai/projects/ascend/zh-"
"cn/latest/user_guide/support_matrix/supported_models.html) to get the "
"model's supported feature matrix."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:12
msgid ""
"Refer to [feature guide](https://docs.vllm.ai/projects/ascend/zh-"
"cn/latest/user_guide/feature_guide/index.html) to get the feature's "
"configuration."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:14
msgid "Environment Preparation"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:15
msgid "Model Weight"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:17
msgid ""
"`Qwen3-Omni-30B-A3B-Thinking` require 2 NPU Card(64G × 2).[Download model"
" weight](https://modelscope.cn/models/Qwen/Qwen3-Omni-30B-A3B-Thinking) "
"It is recommended to download the model weight to the shared directory of"
" multiple nodes, such as `/root/.cache/`"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:20
msgid "Installation"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md
msgid "Use docker image"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:26
msgid ""
"You can using our official docker image to run Qwen3-Omni-30B-A3B-"
"Thinking directly"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:28
msgid ""
"Select an image based on your machine type and start the docker image on "
"your node, refer to [using docker](../installation.md#set-up-using-"
"docker)."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md
msgid "Build from source"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:63
msgid "You can build all from source."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:65
msgid ""
"Install `vllm-ascend`, refer to [set up using python](../installation.md"
"#set-up-using-python)."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:69
msgid "Please install system dependencies"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:79
msgid "Deployment"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:80
msgid "Single-node Deployment"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:81
msgid "Offline Inference on Multi-NPU"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:83
msgid "Run the following script to execute offline inference on multi-NPU:"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:170
msgid "Online Inference on Multi-NPU"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:172
msgid ""
"Run the following script to start the vLLM server on Multi-NPU: For an "
"Atlas A2 with 64 GB of NPU card memory, tensor-parallel-size should be at"
" least 1, and for 32 GB of memory, tensor-parallel-size should be at "
"least 2."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:179
msgid "Functional Verification"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:180
msgid "Once your server is started, you can query the model with input prompts."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:221
msgid "Accuracy Evaluation"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:223
msgid "Here are accuracy evaluation methods."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:225
msgid "Using EvalScope"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:227
msgid ""
"As an example, take the `gsm8k` `omnibench` `bbh` dataset as a test "
"dataset, and run accuracy evaluation of `Qwen3-Omni-30B-A3B-Thinking` in "
"online mode."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:228
msgid ""
"Refer to Using "
"evalscope(https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/evaluation/using_evalscope.html"
"#install-evalscope-using-pip) for `evalscope`installation."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:229
msgid "Run `evalscope` to execute the accuracy evaluation."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:244
#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:283
msgid ""
"After execution, you can get the result, here is the result of `Qwen3"
"-Omni-30B-A3B-Thinking` in vllm-ascend:0.13.0rc1 for reference only."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:258
msgid "Performance"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:260
msgid "Using vLLM Benchmark"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:261
msgid ""
"Run performance evaluation of `Qwen3-Omni-30B-A3B-Thinking` as an "
"example. Refer to vllm benchmark for more details. Refer to [vllm "
"benchmark](https://docs.vllm.ai/en/latest/contributing/benchmarks.html) "
"for more details."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:265
msgid "There are three `vllm bench` subcommand:"
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:266
msgid "`latency`: Benchmark the latency of a single batch of requests."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:267
msgid "`serve`: Benchmark the online serving throughput."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:268
msgid "`throughput`: Benchmark offline inference throughput."
msgstr ""

#: ../../source/tutorials/Qwen3-Omni-30B-A3B-Thinking.md:270
msgid "Take the `serve` as an example. Run the code as follows."
msgstr ""

