# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-07-18 09:01+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.17.0\n"

#: ../../tutorials/multi_npu_quantization.md:1
msgid "Multi-NPU (QwQ 32B W8A8)"
msgstr "多NPU（QwQ 32B W8A8）"

#: ../../tutorials/multi_npu_quantization.md:3
msgid "Run docker container"
msgstr "运行 docker 容器"

#: ../../tutorials/multi_npu_quantization.md:5
msgid "w8a8 quantization feature is supported by v0.8.4rc2 or higher"
msgstr "w8a8 量化功能由 v0.8.4rc2 或更高版本支持"

#: ../../tutorials/multi_npu_quantization.md:31
msgid "Install modelslim and convert model"
msgstr "安装 modelslim 并转换模型"

#: ../../tutorials/multi_npu_quantization.md:33
msgid ""
"You can choose to convert the model yourself or use the quantized model we "
"uploaded,  see https://www.modelscope.cn/models/vllm-ascend/QwQ-32B-W8A8"
msgstr ""
"你可以选择自己转换模型，或者使用我们上传的量化模型，详见 https://www.modelscope.cn/models/vllm-"
"ascend/QwQ-32B-W8A8"

#: ../../tutorials/multi_npu_quantization.md:56
msgid "Verify the quantized model"
msgstr "验证量化模型"

#: ../../tutorials/multi_npu_quantization.md:57
msgid "The converted model files looks like:"
msgstr "转换后的模型文件如下所示："

#: ../../tutorials/multi_npu_quantization.md:70
msgid ""
"Run the following script to start the vLLM server with quantized model:"
msgstr "运行以下脚本以启动带有量化模型的 vLLM 服务器："

#: ../../tutorials/multi_npu_quantization.md:73
msgid ""
"The value \"ascend\" for \"--quantization\" argument will be supported after"
" [a specific PR](https://github.com/vllm-project/vllm-ascend/pull/877) is "
"merged and released, you can cherry-pick this commit for now."
msgstr ""
"在 [特定的PR](https://github.com/vllm-project/vllm-ascend/pull/877) 合并并发布后， \"--"
"quantization\" 参数将支持值 \"ascend\"，你也可以现在手动挑选该提交。"

#: ../../tutorials/multi_npu_quantization.md:79
msgid ""
"Once your server is started, you can query the model with input prompts"
msgstr "一旦服务器启动，就可以通过输入提示词来查询模型。"

#: ../../tutorials/multi_npu_quantization.md:93
msgid ""
"Run the following script to execute offline inference on multi-NPU with "
"quantized model:"
msgstr "运行以下脚本，在多NPU上使用量化模型执行离线推理："

#: ../../tutorials/multi_npu_quantization.md:96
msgid "To enable quantization for ascend, quantization method must be \"ascend\""
msgstr "要在ascend上启用量化，量化方法必须为“ascend”。"
