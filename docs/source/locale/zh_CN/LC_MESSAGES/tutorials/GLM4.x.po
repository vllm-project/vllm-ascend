# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-21 10:23+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/tutorials/GLM4.x.md:1
msgid "GLM-4.5/4.6/4.7"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:3
msgid "Introduction"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:5
msgid ""
"GLM-4.x series models use a Mixture-of-Experts (MoE) architecture and are"
" foundational models specifically designed for agent applications"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:7
msgid "The `GLM-4.5` model is first supported in `vllm-ascend:v0.10.0rc1`"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:9
msgid ""
"This document will show the main verification steps of the model, "
"including supported features, feature configuration, environment "
"preparation, single-node and multi-node deployment, accuracy and "
"performance evaluation."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:11
msgid "Supported Features"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:13
msgid ""
"Refer to [supported "
"features](../user_guide/support_matrix/supported_models.md) to get the "
"model's supported feature matrix."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:15
msgid ""
"Refer to [feature guide](../user_guide/feature_guide/index.md) to get the"
" feature's configuration."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:17
msgid "Environment Preparation"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:19
msgid "Model Weight"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:20
msgid ""
"`GLM-4.5`(BF16 version): [Download model "
"weight](https://www.modelscope.cn/models/ZhipuAI/GLM-4.5)."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:21
msgid ""
"`GLM-4.6`(BF16 version): [Download model "
"weight](https://www.modelscope.cn/models/ZhipuAI/GLM-4.6)."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:22
msgid ""
"`GLM-4.7`(BF16 version): [Download model "
"weight](https://www.modelscope.cn/models/ZhipuAI/GLM-4.7)."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:23
msgid ""
"`GLM-4.5-w8a8-with-float-mtp`(Quantized version with mtp): [Download "
"model weight](https://modelers.cn/models/Modelers_Park/GLM-4.5-w8a8)."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:24
msgid ""
"`GLM-4.6-w8a8`(Quantized version without mtp): [Download model "
"weight](https://modelers.cn/models/Modelers_Park/GLM-4.6-w8a8). Because "
"vllm do not support GLM4.6 mtp in October, so we do not provide mtp "
"version. And last month, it supported, you can use the following "
"quantization scheme to add mtp weights to Quantized weights."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:25
msgid ""
"`Method of Quantify`: [quantization "
"scheme](https://blog.csdn.net/qq_37368095/article/details/156429653?spm=1011.2124.3001.6209)."
" You can use these methods to quantify the model."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:27
msgid ""
"It is recommended to download the model weight to the shared directory of"
" multiple nodes, such as `/root/.cache/`"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:29
msgid "Installation"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:31
msgid "You can using our official docker image to run `GLM-4.x` directly."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:33
msgid ""
"Select an image based on your machine type and start the docker image on "
"your node, refer to [using docker](../installation.md#set-up-using-"
"docker)."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:71
msgid "Deployment"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:73
msgid "Single-node Deployment"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:75
msgid "In low-latency scenarios, we recommend a single-machine deployment."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:76
msgid ""
"Quantized model `glm4.5_w8a8_with_float_mtp` can be deployed on 1 Atlas "
"800 A3 (64G × 16) or 1 Atlas 800 A2 (64G × 8)."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:78
msgid "Run the following script to execute online inference."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:103
msgid "**Notice:** The parameters are explained as follows:"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:105
msgid ""
"For single-node deployment, we recommend using `dp1tp16` and turn off "
"expert parallel in low-latency scenarios."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:106
msgid ""
"`--async-scheduling` Asynchronous scheduling is a technique used to "
"optimize inference efficiency. It allows non-blocking task scheduling to "
"improve concurrency and throughput, especially when processing large-"
"scale models."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:108
msgid "Multi-node Deployment"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:110
msgid "Not recommended to deploy multi-node on Atlas 800 A2 (64G * 8)."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:112
msgid "Prefill-Decode Disaggregation"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:114 ../../source/tutorials/GLM4.x.md:132
msgid "Not test yet."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:116
msgid "Accuracy Evaluation"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:118
msgid "Here are two accuracy evaluation methods."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:120 ../../source/tutorials/GLM4.x.md:136
msgid "Using AISBench"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:121
msgid ""
"Refer to [Using "
"AISBench](../developer_guide/evaluation/using_ais_bench.md) for details."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:123
msgid ""
"After execution, you can get the result, here is the result of `GLM4.6` "
"in `vllm-ascend:main` (after `vllm-ascend:0.13.0rc1`) for reference only."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "dataset"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "version"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "metric"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "mode"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "vllm-api-general-chat"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "note"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "gsm8k"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "-"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "accuracy"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "gen"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "96.13"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "1 Atlas 800 A3 (64G × 16)"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "96.06"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:35
msgid "GPU"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:130
msgid "Using Language Model Evaluation Harness"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:134
msgid "Performance"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:138
msgid ""
"Refer to [Using AISBench for performance "
"evaluation](../developer_guide/evaluation/using_ais_bench.md#execute-"
"performance-evaluation) for details."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:140
msgid "Using vLLM Benchmark"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:142
msgid "Run performance evaluation of `GLM-4.x` as an example."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:144
msgid ""
"Refer to [vllm "
"benchmark](https://docs.vllm.ai/en/latest/contributing/benchmarks.html) "
"for more details."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:146
msgid "There are three `vllm bench` subcommand:"
msgstr ""

#: ../../source/tutorials/GLM4.x.md:147
msgid "`latency`: Benchmark the latency of a single batch of requests."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:148
msgid "`serve`: Benchmark the online serving throughput."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:149
msgid "`throughput`: Benchmark offline inference throughput."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:151
msgid "Take the `serve` as an example. Run the code as follows."
msgstr ""

#: ../../source/tutorials/GLM4.x.md:173
msgid ""
"After about several minutes, you can get the performance evaluation "
"result."
msgstr ""

