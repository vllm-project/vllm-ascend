# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/tutorials/Qwen3-Dense.md:1
msgid "Qwen3-Dense(Qwen3-0.6B/8B/32B)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:3
msgid "Introduction"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:5
msgid ""
"Qwen3 is the latest generation of large language models in Qwen series, "
"offering a comprehensive suite of dense and mixture-of-experts (MoE) "
"models. Built upon extensive training, Qwen3 delivers groundbreaking "
"advancements in reasoning, instruction-following, agent capabilities, and"
" multilingual support."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:7
msgid ""
"Welcome to the tutorial on optimizing Qwen Dense models in the vLLM-"
"Ascend environment. This guide will help you configure the most effective"
" settings for your use case, with practical examples that highlight key "
"optimization points. We will also explore how adjusting service "
"parameters can maximize throughput performance across various scenarios."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:9
msgid ""
"This document will show the main verification steps of the model, "
"including supported features, feature configuration, environment "
"preparation, accuracy and performance evaluation."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:11
msgid ""
"The Qwen3 Dense models is first supported in "
"[v0.8.4rc2](https://github.com/vllm-project/vllm-"
"ascend/blob/main/docs/source/user_guide/release_notes.md#v084rc2---"
"20250429)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:13
#: ../../source/tutorials/Qwen3-Dense.md:116
#: ../../source/tutorials/Qwen3-Dense.md:193
#: ../../source/tutorials/Qwen3-Dense.md:221
#: ../../source/tutorials/Qwen3-Dense.md:299
msgid "**Node**"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:14
msgid ""
"This example requires version **v0.11.0rc2**. Earlier versions may lack "
"certain features."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:16
msgid "Supported Features"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:18
msgid ""
"Refer to [supported "
"features](../user_guide/support_matrix/supported_models.md) to get the "
"model's supported feature matrix."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:20
msgid ""
"Refer to [feature guide](../user_guide/feature_guide/index.md) to get the"
" feature's configuration."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:22
msgid "Environment Preparation"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:24
msgid "Model Weight"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:26
msgid ""
"`Qwen3-0.6B`(BF16 version): require 1 Atlas 800 A3 (64G × 2) card or 1 "
"Atlas 800I A2 (64G × 1) card. [Download model "
"weight](https://modelers.cn/models/Modelers_Park/Qwen3-0.6B)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:27
msgid ""
"`Qwen3-1.7B`(BF16 version): require 1 Atlas 800 A3 (64G × 2) card or 1 "
"Atlas 800I A2 (64G × 1) card. [Download model "
"weight](https://modelers.cn/models/Modelers_Park/Qwen3-1.7B)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:28
msgid ""
"`Qwen3-4B`(BF16 version): require 1 Atlas 800 A3 (64G × 2) card or 1 "
"Atlas 800I A2 (64G × 1) card. [Download model "
"weight](https://modelers.cn/models/Modelers_Park/Qwen3-4B)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:29
msgid ""
"`Qwen3-8B`(BF16 version): require 1 Atlas 800 A3 (64G × 2) card or 1 "
"Atlas 800I A2 (64G × 1) card. [Download model "
"weight](https://modelers.cn/models/Modelers_Park/Qwen3-8B)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:30
msgid ""
"`Qwen3-14B`(BF16 version): require 1 Atlas 800 A3 (64G × 2) card or 2 "
"Atlas 800I A2 (64G × 1) cards. [Download model "
"weight](https://modelers.cn/models/Modelers_Park/Qwen3-14B)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:31
msgid ""
"`Qwen3-32B`(BF16 version): require 2 Atlas 800 A3 (64G × 4) cards or 4 "
"Atlas 800I A2 (64G × 4) cards. [Download model "
"weight](https://modelers.cn/models/Modelers_Park/Qwen3-32B)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:32
msgid ""
"`Qwen3-32B-W8A8`(Quantized version): require 2 Atlas 800 A3 (64G × 4) "
"cards or 4 Atlas 800I A2 (64G × 4) cards. [Download model "
"weight](https://www.modelscope.cn/models/vllm-ascend/Qwen3-32B-W8A8)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:34
msgid ""
"These are the recommended numbers of cards, which can be adjusted "
"according to the actual situation."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:36
msgid ""
"It is recommended to download the model weight to the shared directory of"
" multiple nodes, such as `/root/.cache/`"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:38
msgid "Verify Multi-node Communication(Optional)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:40
msgid ""
"If you want to deploy multi-node environment, you need to verify multi-"
"node communication according to [verify multi-node communication "
"environment](../installation.md#verify-multi-node-communication)."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:42
msgid "Installation"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:44
msgid ""
"You can using our official docker image for supporting Qwen3 Dense "
"models. Currently, we provide the all-in-one images.[Download "
"images](https://quay.io/repository/ascend/vllm-ascend?tab=tags)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:47
msgid "Docker Pull (by tag)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:56
msgid "Docker run"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:93
msgid ""
"The default workdir is `/workspace`, vLLM and vLLM Ascend code are placed"
" in `/vllm-workspace` and installed in [development "
"mode](https://setuptools.pypa.io/en/latest/userguide/development_mode.html)"
" (`pip install -e`) to help developer immediately take place changes "
"without requiring a new installation."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:95
msgid ""
"In the [Run docker container](./Qwen3-Dense.md#run-docker-container), "
"detailed explanations are provided through specific examples."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:97
msgid ""
"In addition, if you don't want to use the docker image as above, you can "
"also build all from source:"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:99
msgid ""
"Install `vllm-ascend` from source, refer to "
"[installation](../installation.md)."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:101
msgid ""
"If you want to deploy multi-node environment, you need to set up "
"environment on each node."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:103
msgid "Deployment"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:105
msgid ""
"In this section, we will demonstrate best practices for adjusting "
"hyperparameters in vLLM-Ascend to maximize inference throughput "
"performance. By tailoring service-level configurations to fit different "
"use cases, you can ensure that your system performs optimally across "
"various scenarios. We will guide you through how to fine-tune "
"hyperparameters based on observed phenomena, such as max_model_len, "
"max_num_batched_tokens, and cudagraph_capture_sizes, to achieve the best "
"performance."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:107
msgid "The specific example scenario is as follows:"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:108
msgid "The machine environment is an Atlas 800 A3 (64G*16)"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:109
msgid "The LLM is Qwen3-32B-W8A8"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:110
msgid "The data scenario is a fixed-length input of 3.5K and an output of 1.5K."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:111
msgid "The parallel configuration requirement is DP=1&TP=4"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:112
msgid ""
"If the machine environment is an **Atlas 800I A2(64G*8)**, the deployment"
" approach stays identical."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:114
msgid "Run docker container:"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:117
#: ../../source/tutorials/Qwen3-Dense.md:194
#: ../../source/tutorials/Qwen3-Dense.md:222
#: ../../source/tutorials/Qwen3-Dense.md:300
msgid ""
"/model/Qwen3-32B-W8A8 is the model path, replace this with your actual "
"path."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:118
msgid "v0.11.0rc2-a3 is image tag, replace this with your actual tag."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:119
msgid "replace this with your actual port: '-p 8113:8113'."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:120
msgid "replace this with your actual card: '--device /dev/davinci0'."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:147
msgid "Online Inference on Multi-NPU"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:149
msgid "Run the following script to start the vLLM server on Multi-NPU."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:151
msgid ""
"This script is configured to achieve optimal performance under the above "
"specific example scenarios,with batchsize = 72 on two A3 cards."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:196
msgid ""
"If the model is not a quantized model, remove the `--quantization ascend`"
" parameter."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:198
#, python-brace-format
msgid ""
"**[Optional]** `--additional-config '{\"pa_shape_list\":[48,64,72,80]}'`:"
" `pa_shape_list` specifies the batch sizes where you want to switch to "
"the PA operator. This is a temporary tuning knob. Currently, the "
"attention operator dispatch defaults to the FIA operator. In some batch-"
"size (concurrency) settings, FIA may have suboptimal performance. By "
"setting `pa_shape_list`, when the runtime batch size matches one of the "
"listed values, vLLM-Ascend will replace FIA with the PA operator to "
"prevent performance degradation. In the future, FIA will be optimized for"
" these scenarios and this parameter will be removed."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:200
#, python-brace-format
msgid ""
"If the ultimate performance is desired, the cudagraph_capture_sizes "
"parameter can be enabled, reference: [key-optimization-"
"points](./Qwen3-Dense.md#key-optimization-points)、[optimization-"
"highlights](./Qwen3-Dense.md#optimization-highlights). Here is an example"
" of batchsize of 72: `--compilation-config '{\"cudagraph_mode\": "
"\"FULL_DECODE_ONLY\", "
"\"cudagraph_capture_sizes\":[1,8,24,48,60,64,72,76]}'`."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:202
msgid "Once your server is started, you can query the model with input prompts"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:217
msgid "Offline Inference on Multi-NPU"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:219
msgid "Run the following script to execute offline inference on multi-NPU."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:224
msgid ""
"If the model is not a quantized model,remove the "
"`quantization=\"ascend\"` parameter."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:264
msgid "Accuracy Evaluation"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:266
msgid "Here is one accuracy evaluation methods."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:268
#: ../../source/tutorials/Qwen3-Dense.md:282
msgid "Using AISBench"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:270
msgid ""
"Refer to [Using "
"AISBench](../developer_guide/evaluation/using_ais_bench.md) for details."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:272
msgid ""
"After execution, you can get the result, here is the result of `Qwen3"
"-32B-W8A8` in `vllm-ascend:0.11.0rc2` for reference only."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "dataset"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "version"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "metric"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "mode"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "task name"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "vllm-api-general-chat"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "gsm8k"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "-"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "accuracy"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "gen"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "gsm8k_gen_0_shot_noncot_chat_prompt"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "96.44"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "math500"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "math500_gen_0_shot_cot_chat_prompt"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "97.60"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "aime"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "aime2024_gen_0_shot_chat_prompt"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:122
msgid "76.67"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:280
msgid "Performance"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:284
msgid ""
"Refer to [Using AISBench for performance "
"evaluation](../developer_guide/evaluation/using_ais_bench.md#execute-"
"performance-evaluation) for details."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:286
msgid "Using vLLM Benchmark"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:288
msgid "Run performance evaluation of `Qwen3-32B-W8A8` as an example."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:290
msgid ""
"Refer to [vllm "
"benchmark](https://docs.vllm.ai/en/latest/contributing/benchmarks.html) "
"for more details."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:292
msgid "There are three `vllm bench` subcommand:"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:293
msgid "`latency`: Benchmark the latency of a single batch of requests."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:294
msgid "`serve`: Benchmark the online serving throughput."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:295
msgid "`throughput`: Benchmark offline inference throughput."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:297
msgid "Take the `serve` as an example. Run the code as follows."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:306
msgid ""
"After about several minutes, you can get the performance evaluation "
"result."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:308
msgid "Key Optimization Points"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:309
msgid ""
"In this section, we will cover the key optimization points that can "
"significantly improve the performance of Qwen Dense models. These "
"techniques are designed to enhance throughput and efficiency across "
"various scenarios."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:311
msgid "1. Rope Optimization"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:312
msgid ""
"Rope optimization enhances the model's efficiency by modifying the "
"position encoding process. Specifically, it ensures that the "
"cos_sin_cache and the associated index selection operation are only "
"performed during the first layer of the forward pass. For subsequent "
"layers, the position encoding is directly reused, eliminating redundant "
"calculations and significantly speeding up inference in decode phase."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:314
#: ../../source/tutorials/Qwen3-Dense.md:319
#: ../../source/tutorials/Qwen3-Dense.md:345
msgid ""
"This optimization is enabled by default and does not require any "
"additional environment variables to be set."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:316
msgid "2. AddRMSNormQuant Fusion"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:317
msgid ""
"AddRMSNormQuant fusion merges the Address-wise Multi-Scale Normalization "
"and Quantization operations, allowing for more efficient memory access "
"and computation, thereby enhancing throughput."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:321
msgid "3. FlashComm_v1"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:322
msgid ""
"FlashComm_v1 significantly improves performance in large-batch scenarios "
"by decomposing the traditional allreduce collective communication into "
"reduce-scatter and all-gather. This breakdown helps reduce the "
"computation of the RMSNorm token dimensions, leading to more efficient "
"processing. In quantization scenarios, FlashComm_v1 also reduces the "
"communication overhead by decreasing the bit-level data transfer, which "
"further minimizes the end-to-end latency during the prefill phase."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:324
msgid ""
"It is important to note that the decomposition of the allreduce "
"communication into reduce-scatter and all-gather operations only provides"
" benefits in high-concurrency scenarios, where there is no significant "
"communication degradation. In other cases, this decomposition may result "
"in noticeable performance degradation. To mitigate this, the current "
"implementation uses a threshold-based approach, where FlashComm_v1 is "
"only enabled if the actual token count for each inference schedule "
"exceeds the threshold. This ensures that the feature is only activated in"
" scenarios where it improves performance, avoiding potential degradation "
"in lower-concurrency situations."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:326
msgid ""
"This optimization requires setting the environment variable "
"`VLLM_ASCEND_ENABLE_FLASHCOMM1 = 1` to be enabled."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:328
msgid "4. Matmul and ReduceScatter Fusion"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:329
msgid ""
"Once FlashComm_v1 is enabled, an additional optimization can be applied. "
"This optimization fuses matrix multiplication and ReduceScatter "
"operations, along with tiling optimization. The Matmul computation is "
"treated as one pipeline, while the ReduceScatter and dequant operations "
"are handled in a separate pipeline. This approach significantly reduces "
"communication steps, improves computational efficiency, and allows for "
"better resource utilization, resulting in enhanced throughput, especially"
" in large-scale distributed environments."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:331
msgid ""
"This optimization is automatically enabled once FlashComm_v1 is "
"activated. However, due to an issue with performance degradation in "
"small-concurrency scenarios after this fusion, a threshold-based approach"
" is currently used to mitigate this problem. The optimization is only "
"applied when the token count exceeds the threshold, ensuring that it is "
"not enabled in cases where it could negatively impact performance."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:333
msgid "5. Weight Prefetching"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:334
msgid ""
"Weight prefetching optimizes memory usage by preloading weights into the "
"cache before they are needed, minimizing delays caused by memory access "
"during model execution."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:336
msgid ""
"In dense model scenarios, the MLP's gate_up_proj and down_proj linear "
"layers often exhibit relatively high MTE utilization. To address this, we"
" create a separate pipeline specifically for weight prefetching, which "
"runs in parallel with the original vector computation pipeline, such as "
"RMSNorm and SiLU, before the MLP. This approach allows the weights to be "
"preloaded to L2 cache ahead of time, reducing MTE utilization during the "
"MLP computations and indirectly improving Cube computation efficiency by "
"minimizing resource contention and optimizing data flow."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:338
msgid ""
"It is important to emphasize that, since we use vector computations to "
"hide the weight prefetching pipeline, the setting of the prefetch buffer "
"size is crucial. If the buffer size is too small, the optimization "
"benefits will not be fully realized, while a larger buffer size may lead "
"to resource contention, resulting in performance degradation. To "
"accommodate different scenarios, we have exposed two environment "
"variables `VLLM_ASCEND_MLP_GATE_UP_PREFETCH_SIZE` and "
"`VLLM_ASCEND_MLP_DOWN_PREFETCH_SIZE` to allow for flexible buffer size "
"configuration based on the specific workload."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:340
msgid ""
"This optimization requires setting the environment variable "
"`VLLM_ASCEND_ENABLE_PREFETCH_MLP = 1` to be enabled."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:342
msgid "6. Zerolike Elimination"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:343
msgid ""
"This elimination removes unnecessary operations related to zero-like "
"tensors in Attention forward, improving the efficiency of matrix "
"operations and reducing memory usage."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:347
msgid "7. FullGraph Optimization"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:348
msgid ""
"ACLGraph offers several key optimizations to improve model execution "
"efficiency. By replaying the entire model execution graph at once, we "
"significantly reduce dispatch latency compared to multiple smaller "
"replays. This approach also stabilizes multi-device performance, as "
"capturing the model as a single static graph mitigates dispatch "
"fluctuations across devices. Additionally, consolidating graph captures "
"frees up streams, allowing for the capture of more graphs and optimizing "
"resource usage, ultimately leading to improved system efficiency and "
"reduced overhead."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:350
#, python-brace-format
msgid ""
"The configuration compilation_config = { \"cudagraph_mode\": "
"\"FULL_DECODE_ONLY\"} is used when starting the service. This setup is "
"necessary to enable the aclgraph's full decode-only mode."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:352
msgid "8. Asynchronous Scheduling"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:353
msgid ""
"Asynchronous scheduling is a technique used to optimize inference "
"efficiency. It allows non-blocking task scheduling to improve concurrency"
" and throughput, especially when processing large-scale models."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:355
msgid "This optimization is enabled by setting `--async-scheduling`."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:357
msgid "Optimization Highlights"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:359
msgid ""
"Building on the specific example scenarios outlined earlier, this section"
" highlights the key tuning points that played a crucial role in achieving"
" optimal performance. By focusing on the most impactful adjustments to "
"hyperparameters and optimizations, we’ll emphasize the strategies that "
"can be leveraged to maximize throughput, minimize latency, and ensure "
"efficient resource utilization in various environments. These insights "
"will help guide you in fine-tuning your own configurations for the best "
"possible results."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:361
msgid "1.Prefetch Buffer Size"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:362
msgid ""
"Setting the right prefetch buffer size is essential for optimizing weight"
" loading and the size of this prefetch buffer is directly related to the "
"time that can be hidden by vector computations. To achieve a near-perfect"
" overlap between the prefetch and computation streams, you can flexibly "
"adjust the buffer size by profiling and observing the degree of overlap "
"at different buffer sizes."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:364
msgid ""
"For example, in the real-world scenario mentioned above, I set the "
"prefetch buffer size for the gate_up_proj and down_proj in the MLP to "
"18MB. The reason for this is that, at this value, the vector computations"
" of RMSNorm and SiLU can effectively hide the prefetch stream, thereby "
"accelerating the Matmul computations of the two linear layers."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:366
msgid "2.Max-num-batched-tokens"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:367
msgid ""
"The max-num-batched-tokens parameter determines the maximum number of "
"tokens that can be processed in a single batch. Adjusting this value "
"helps to balance throughput and memory usage. Setting this value too "
"small can negatively impact end-to-end performance, as fewer tokens are "
"processed per batch, potentially leading to inefficiencies. Conversely, "
"setting it too large increases the risk of Out of Memory (OOM) errors due"
" to excessive memory consumption."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:369
msgid ""
"In the above real-world scenario, we not only conducted extensive testing"
" to determine the most cost-effective value, but also took into account "
"the accumulation of decode tokens when enabling chunked prefill. If the "
"value is set too small, a single request may be chunked multiple times, "
"and during the early stages of inference, a batch may contain only a "
"small number of decode tokens. This can result in the end-to-end "
"throughput falling short of expectations."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:371
msgid "3.Cudagraph_capture_sizes"
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:372
msgid ""
"The cudagraph_capture_sizes parameter controls the granularity of graph "
"captures during the inference process. Adjusting this value determines "
"how much of the computation graph is captured at once, which can "
"significantly impact both performance and memory usage."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:374
msgid ""
"If this list is not manually specified, it will be filled with a series "
"of evenly distributed values, which typically ensures good performance. "
"However, if you want to fine-tune it further, manually specifying the "
"values will yield better results. This is because if the batch size falls"
" between two sizes, the framework will automatically pad the token count "
"to the larger size. This often leads to actual performance deviating from"
" the expected or even degrading."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:376
msgid ""
"Therefore, like the above real-world scenario, when adjusting the "
"benchmark request concurrency, we always ensure that the concurrency is "
"actually included in the cudagraph_capture_sizes list. This way, during "
"the decode phase, padding operations are essentially avoided, ensuring "
"the reliability of the experimental data."
msgstr ""

#: ../../source/tutorials/Qwen3-Dense.md:378
msgid ""
"It’s important to note that if you enable FlashComm_v1, the values in "
"this list must be integer multiples of the TP size. Any values that do "
"not meet this condition will be automatically filtered out. Therefore, I "
"recommend incrementally adding concurrency based on the TP size after "
"enabling FlashComm_v1."
msgstr ""

