# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/tutorials/Qwen-VL-Dense.md:1
msgid "Qwen-VL-Dense(Qwen2.5VL-3B/7B, Qwen3-VL-2B/4B/8B/32B)"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:3
msgid "Introduction"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:5
msgid ""
"The Qwen-VL(Vision-Language)series from Alibaba Cloud comprises a family "
"of powerful Large Vision-Language Models (LVLMs) designed for "
"comprehensive multimodal understanding. They accept images, text, and "
"bounding boxes as input, and output text and detection boxes, enabling "
"advanced functions like image detection, multi-modal dialogue, and multi-"
"image reasoning."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:7
msgid ""
"This document will show the main verification steps of the model, "
"including supported features, feature configuration, environment "
"preparation, NPU deployment, accuracy and performance evaluation."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:9
msgid ""
"This tutorial uses the vLLM-Ascend `v0.11.0rc3-a3` version for "
"demonstration, showcasing the `Qwen3-VL-8B-Instruct` model as an example "
"for single NPU deployment and the `Qwen2.5-VL-32B-Instruct` model as an "
"example for multi-NPU deployment."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:11
msgid "Supported Features"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:13
msgid ""
"Refer to [supported "
"features](../user_guide/support_matrix/supported_models.md) to get the "
"model's supported feature matrix."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:15
msgid ""
"Refer to [feature guide](../user_guide/feature_guide/index.md) to get the"
" feature's configuration."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:17
msgid "Environment Preparation"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:19
msgid "Model Weight"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:21
msgid "require 1 Atlas 800I A2 (64G × 8) node or 1 Atlas 800 A3 (64G × 16) node:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:22
msgid ""
"`Qwen2.5-VL-3B-Instruct`: [Download model "
"weight](https://modelscope.cn/models/Qwen/Qwen2.5-VL-3B-Instruct)"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:23
msgid ""
"`Qwen2.5-VL-7B-Instruct`: [Download model "
"weight](https://modelscope.cn/models/Qwen/Qwen2.5-VL-7B-Instruct)"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:24
msgid ""
"`Qwen2.5-VL-32B-Instruct`:[Download model "
"weight](https://modelscope.cn/models/Qwen/Qwen2.5-VL-32B-Instruct)"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:25
msgid ""
"`Qwen2.5-VL-72B-Instruct`:[Download model "
"weight](https://modelscope.cn/models/Qwen/Qwen2.5-VL-72B-Instruct)"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:26
msgid ""
"`Qwen3-VL-2B-Instruct`:   [Download model "
"weight](https://modelscope.cn/models/Qwen/Qwen3-VL-2B-Instruct)"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:27
msgid ""
"`Qwen3-VL-4B-Instruct`:   [Download model "
"weight](https://modelscope.cn/models/Qwen/Qwen3-VL-4B-Instruct)"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:28
msgid ""
"`Qwen3-VL-8B-Instruct`:   [Download model "
"weight](https://modelscope.cn/models/Qwen/Qwen3-VL-8B-Instruct)"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:29
msgid ""
"`Qwen3-VL-32B-Instruct`:  [Download model "
"weight](https://modelscope.cn/models/Qwen/Qwen3-VL-32B-Instruct)"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:31
msgid ""
"A sample Qwen2.5-VL quantization script can be found in the modelslim "
"code repository. [Qwen2.5-VL Quantization Script "
"Example](https://gitcode.com/Ascend/msit/blob/master/msmodelslim/example/multimodal_vlm/Qwen2.5-VL/README.md)"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:33
msgid ""
"It is recommended to download the model weight to the shared directory of"
" multiple nodes, such as `/root/.cache/`"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:35
msgid "Installation"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md
msgid "single-NPU"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:44
#: ../../source/tutorials/Qwen-VL-Dense.md:72
msgid "Run docker container:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md
msgid "multi-NPU"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:100
msgid "Setup environment variables:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:111
msgid ""
"`max_split_size_mb` prevents the native allocator from splitting blocks "
"larger than this size (in MB). This can reduce fragmentation and may "
"allow some borderline workloads to complete without running out of "
"memory. You can find more details "
"[<u>here</u>](https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha003/apiref/envref/envref_07_0061.html)."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:114
msgid "Deployment"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:116
msgid "Offline Inference"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md
msgid "Qwen3-VL-8B-Instruct"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:125
msgid "Run the following script to execute offline inference on single-NPU:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:190
#: ../../source/tutorials/Qwen-VL-Dense.md:286
msgid "If you run this script successfully, you can see the info shown below:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md
msgid "Qwen2.5-VL-32B-Instruct"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:220
msgid "Run the following script to execute offline inference on multi-NPU:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:311
msgid "Online Serving"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:320
msgid "Run docker container to start the vLLM server on single-NPU:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:331
msgid ""
"Add `--max_model_len` option to avoid ValueError that the Qwen3-VL-8B-"
"Instruct model's max seq len (256000) is larger than the maximum number "
"of tokens that can be stored in KV cache. This will differ with different"
" NPU series base on the HBM size. Please modify the value according to a "
"suitable value for your NPU series."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:334
#: ../../source/tutorials/Qwen-VL-Dense.md:421
msgid "If your service start successfully, you can see the info shown below:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:342
#: ../../source/tutorials/Qwen-VL-Dense.md:429
msgid "Once your server is started, you can query the model with input prompts:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:359
#: ../../source/tutorials/Qwen-VL-Dense.md:446
msgid ""
"If you query the server successfully, you can see the info shown below "
"(client):"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:365
#: ../../source/tutorials/Qwen-VL-Dense.md:452
msgid "Logs of the vllm server:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:380
msgid "Run docker container to start the vLLM server on multi-NPU:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:418
msgid ""
"Add `--max_model_len` option to avoid ValueError that the Qwen2.5-VL-32B-"
"Instruct model's max_model_len (128000) is larger than the maximum number"
" of tokens that can be stored in KV cache. This will differ with "
"different NPU series base on the HBM size. Please modify the value "
"according to a suitable value for your NPU series."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:467
msgid "Accuracy Evaluation"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:469
msgid "Using Language Model Evaluation Harness"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:471
msgid ""
"The accuracy of some models is already within our CI monitoring scope, "
"including:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:472
msgid "`Qwen2.5-VL-7B-Instruct`"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:473
msgid "`Qwen3-VL-8B-Instruct`"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:475
msgid ""
"You can refer to the [monitoring configuration](https://github.com/vllm-"
"project/vllm-"
"ascend/blob/main/.github/workflows/vllm_ascend_test_nightly_a2.yaml)."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:484
msgid ""
"As an example, take the `mmmu_val` dataset as a test dataset, and run "
"accuracy evaluation of `Qwen3-VL-8B-Instruct` in offline mode."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:486
#: ../../source/tutorials/Qwen-VL-Dense.md:517
msgid ""
"Refer to [Using lm_eval](../developer_guide/evaluation/using_lm_eval.md) "
"for more details on `lm_eval` installation."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:492
#: ../../source/tutorials/Qwen-VL-Dense.md:523
msgid "Run `lm_eval` to execute the accuracy evaluation."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:505
msgid ""
"After execution, you can get the result, here is the result of `Qwen3-VL-"
"8B-Instruct` in `vllm-ascend:0.11.0rc3` for reference only."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:515
msgid ""
"As an example, take the `mmmu_val` dataset as a test dataset, and run "
"accuracy evaluation of `Qwen2.5-VL-32B-Instruct` in offline mode."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:535
msgid ""
"After execution, you can get the result, here is the result of `Qwen2.5"
"-VL-32B-Instruct` in `vllm-ascend:0.11.0rc3` for reference only."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:543
msgid "Performance"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:545
msgid "Using vLLM Benchmark"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:547
msgid ""
"Refer to [vllm "
"benchmark](https://docs.vllm.ai/en/latest/contributing/benchmarks.html) "
"for more details."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:549
msgid "There are three `vllm bench` subcommand:"
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:550
msgid "`latency`: Benchmark the latency of a single batch of requests."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:551
msgid "`serve`: Benchmark the online serving throughput."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:552
msgid "`throughput`: Benchmark offline inference throughput."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:554
msgid ""
"The performance evaluation must be conducted in an online mode. Take the "
"`serve` as an example. Run the code as follows."
msgstr ""

#: ../../source/tutorials/Qwen-VL-Dense.md:577
msgid ""
"After about several minutes, you can get the performance evaluation "
"result."
msgstr ""

