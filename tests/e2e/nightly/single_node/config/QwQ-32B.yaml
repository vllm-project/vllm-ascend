# ==========================================
# Shared Configurations
# ==========================================

_envs: &envs
  TASK_QUEUE_ENABLE: "1"
  OMP_PROC_BIND: "false"
  HCCL_OP_EXPANSION_MODE: "AIV"
  VLLM_ASCEND_ENABLE_FLASHCOMM: "1"
  VLLM_ASCEND_ENABLE_DEBSE_OPTIMIZE: "1"
  SERVER_PORT: "8080"

_server_cmd: &server_cmd
  - "--tensor-parallel-size"
  - "4"
  - "--port"
  - "$SERVER_PORT"
  - "--max-model-len"
  - "36864"
  - "--max-num-batched-tokens"
  - "36864"
  - "--block-size"
  - "128"
  - "--trust-remote-code"
  - "--gpu-memory-utilization"
  - "0.9"
  - "--reasoning-parser"
  - "deepseek_r1"
  - "--distributed_executor_backend"
  - "mp"
  - "--additional-config"
  - '{"weight_prefetch_config":{"enabled":true}}'

_benchmarks: &benchmarks
  acc:
    case_type: accuracy
    dataset_path: vllm-ascend/gsm8k-lite
    request_conf: vllm_api_general_chat
    dataset_conf: gsm8k/gsm8k_gen_0_shot_cot_chat_prompt
    max_out_len: 32768
    batch_size: 32
    baseline: 95
    threshold: 5
  perf:
    case_type: performance
    dataset_path: vllm-ascend/GSM8K-in3500-bs400
    request_conf: vllm_api_stream_chat
    dataset_conf: gsm8k/gsm8k_gen_0_shot_cot_str_perf
    num_prompts: 240
    max_out_len: 1500
    batch_size: 60
    baseline: 1
    threshold: 0.97

# ==========================================
# ACTUAL TEST CASES
# ==========================================

test_cases:
  - name: "QwQ-32B-aclgraph"
    model: "Qwen/QwQ-32B"
    envs:
      <<: *envs
    server_cmd: *server_cmd
    server_cmd_extra:
      - "--compilation_config"
      - '{"cudagraph_mode":"FULL_DECODE_ONLY", "cudagraph_capture_sizes": [1, 8, 24, 48, 60]}'
    benchmarks:
      <<: *benchmarks

  - name: "QwQ-32B-single"
    model: "Qwen/QwQ-32B"
    envs:
      <<: *envs
    server_cmd: *server_cmd
    server_cmd_extra:
      - "--enforce-eager"
    benchmarks:
