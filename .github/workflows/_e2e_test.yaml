name: 'e2e test'

on:
  workflow_call:
    inputs:
      vllm:
        required: true
        type: string
      type:
        required: true
        type: string
      contains_310:
        required: true
        type: boolean
      is_e2e_change:
        required: true
        type: boolean

jobs:
  e2e:
    name: singlecard
    runs-on: linux-aarch64-a2-1
    container:
      image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/cann:8.3.rc2-910b-ubuntu22.04-py3.11
      env:
        VLLM_LOGGING_LEVEL: ERROR
        VLLM_USE_MODELSCOPE: True
        TRANSFORMERS_OFFLINE: 1
    steps:
      - name: Skip e2e test if not change
        if: ${{ !inputs.is_e2e_change }}
        run: |
          echo "The change is not related to e2e test, Skip it."

      - name: Checkout vllm-project/vllm-ascend repo
        if: ${{ inputs.is_e2e_change }}
        uses: actions/checkout@v6

      - name: Prepare environment
        if: ${{ inputs.is_e2e_change }}
        uses: ./.github/actions/e2e_test_pre
        with:
          vllm: ${{ inputs.vllm }}

      - name: Run light e2e test
        env:
          PYTORCH_NPU_ALLOC_CONF: max_split_size_mb:256
          VLLM_WORKER_MULTIPROC_METHOD: spawn
        if: ${{ inputs.type == 'light' && inputs.is_e2e_change }}
        run: |
          pytest -sv --durations=0 tests/e2e/singlecard/test_aclgraph_accuracy.py::test_piecewise_res_consistency
          pytest -sv --durations=0 tests/e2e/singlecard/test_quantization.py::test_qwen3_w8a8_quant

      - name: Run full e2e test
        env:
          VLLM_WORKER_MULTIPROC_METHOD: spawn
          PYTORCH_NPU_ALLOC_CONF: max_split_size_mb:256
        if: ${{ inputs.type == 'full' && inputs.is_e2e_change }}
        run: |
          # We found that if running aclgraph tests in batch, it will cause AclmdlRICaptureBegin error. So we run the test separately.
          # basic
          pytest -sv --durations=0 tests/e2e/singlecard/test_aclgraph_accuracy.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_aclgraph_mem.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_async_scheduling.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_batch_invariant.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_camem.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_completion_with_prompt_embeds.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_cpu_offloading.py
          # xgrammar has parameter mismatching bug, please follows: https://github.com/vllm-project/vllm-ascend/issues/5524
          # pytest -sv --durations=0 tests/e2e/singlecard/test_guided_decoding.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_ilama_lora.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_llama32_lora.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_qwen3_multi_loras.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_models.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_multistream_overlap_shared_expert.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_profile_execute_duration.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_quantization.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_sampler.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_vlm.py
          pytest -sv --durations=0 tests/e2e/singlecard/test_xlite.py

          # compile
          pytest -sv --durations=0 tests/e2e/singlecard/compile/test_norm_quant_fusion.py
  
          # model_runner_v2
          # pytest -sv --durations=0 tests/e2e/singlecard/model_runner_v2/test_basic.py

          # pooling
          pytest -sv --durations=0 tests/e2e/singlecard/pooling/test_classification.py
          pytest -sv --durations=0 tests/e2e/singlecard/pooling/test_embedding.py
          pytest -sv --durations=0 tests/e2e/singlecard/pooling/test_scoring.py

          # spec_decode
          pytest -sv --durations=0 tests/e2e/singlecard/spec_decode/test_mtp_eagle_correctness.py
          pytest -sv --durations=0 tests/e2e/singlecard/spec_decode/test_v1_spec_decode.py
  
  e2e-2-cards:
    name: multicard-2
    runs-on: linux-aarch64-a3-2
    container:
      image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/cann:8.3.rc2-a3-ubuntu22.04-py3.11
      env:
        VLLM_LOGGING_LEVEL: ERROR
        VLLM_USE_MODELSCOPE: True
        HCCL_BUFFSIZE: 1024
        TRANSFORMERS_OFFLINE: 1
    steps:
      - name: Skip e2e test if not change
        if: ${{ !inputs.is_e2e_change }}
        run: |
          echo "The change is not related to e2e test, Skip it."

      - name: Checkout vllm-project/vllm-ascend repo
        if: ${{ inputs.is_e2e_change }}
        uses: actions/checkout@v6

      - name: Prepare environment
        if: ${{ inputs.is_e2e_change }}
        uses: ./.github/actions/e2e_test_pre
        with:
          vllm: ${{ inputs.vllm }}

      - name: Run light e2e test
        env:
          VLLM_WORKER_MULTIPROC_METHOD: spawn
        if: ${{ inputs.type == 'light' }}
        run: |
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_qwen3_moe.py::test_qwen3_moe_distributed_mp_tp2_ep

      - name: Run full e2e test
        env:
          VLLM_WORKER_MULTIPROC_METHOD: spawn
        if: ${{ inputs.type == 'full' }}
        run: |
          # This test will fail with trion ascend intalled. Fix Me.
          # pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_aclgraph_capture_replay.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_data_parallel.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_expert_parallel.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_external_launcher.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_full_graph_mode.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_ilama_lora_tp2.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/spec_decode/test_spec_decode.py

          # To avoid oom, we need to run the test in a single process.
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_offline_inference_distributed.py::test_deepseek_multistream_moe_tp2
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_offline_inference_distributed.py::test_qwen3_w4a8_dynamic_tp2
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_offline_inference_distributed.py::test_qwen3_moe_sp_tp2
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_offline_inference_distributed.py::test_deepseek_w4a8_accuracy_tp2
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_offline_inference_distributed.py::test_qwen3_moe_fc2_tp2
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_offline_inference_distributed.py::test_deepseek_v2_lite_fc1_tp2
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_offline_inference_distributed.py::test_qwen3_dense_fc1_tp2
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_offline_inference_distributed.py::test_qwen3_dense_prefetch_mlp_weight_tp2

          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_offline_weight_load.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_pipeline_parallel.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_prefix_caching.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_quantization.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_qwen3_moe.py
          # This test is broken, fix me
          #pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_shared_expert_dp.py
          pytest -sv --durations=0 tests/e2e/multicard/2-cards/test_single_request_aclgraph.py

  e2e-4-cards:
    name: multicard-4
    needs: [e2e-2-cards]
    if: ${{ needs.e2e-2-cards.result == 'success' && inputs.type == 'full' }}
    runs-on: linux-aarch64-a3-4
    container:
      image: m.daocloud.io/quay.io/ascend/cann:8.3.rc2-a3-ubuntu22.04-py3.11
      env:
        VLLM_LOGGING_LEVEL: ERROR
        VLLM_USE_MODELSCOPE: True
        TRANSFORMERS_OFFLINE: 1
    steps:
      - name: Skip e2e test if not change
        if: ${{ !inputs.is_e2e_change }}
        run: |
          echo "The change is not related to e2e test, Skip it."

      - name: Checkout vllm-project/vllm-ascend repo
        if: ${{ inputs.is_e2e_change }}
        uses: actions/checkout@v6

      - name: Prepare environment
        if: ${{ inputs.is_e2e_change }}
        uses: ./.github/actions/e2e_test_pre
        with:
          vllm: ${{ inputs.vllm }}

      - name: Run full e2e test
        env:
          VLLM_WORKER_MULTIPROC_METHOD: spawn
        run: |
          pytest -sv --durations=0 tests/e2e/multicard/4-cards/test_data_parallel_tp2.py
          pytest -sv --durations=0 tests/e2e/multicard/4-cards/test_kimi_k2.py
          pytest -sv --durations=0 tests/e2e/multicard/4-cards/test_qwen3_next.py 

          # long_sequence
          pytest -sv --durations=0 tests/e2e/multicard/4-cards/long_sequence/test_accuracy.py
          pytest -sv --durations=0 tests/e2e/multicard/4-cards/long_sequence/test_basic.py
          pytest -sv --durations=0 tests/e2e/multicard/4-cards/long_sequence/test_chunked_prefill.py
          pytest -sv --durations=0 tests/e2e/multicard/4-cards/long_sequence/test_mtp.py

          # spec_decode
          pytest -sv --durations=0 tests/e2e/multicard/4-cards/spec_decode/test_mtp_qwen3_next.py

  e2e_310p:
    name: 310p singlecard
    runs-on: linux-aarch64-310p-1
    if: ${{ inputs.contains_310 }}
    container:
      image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/cann:8.3.rc2-310p-ubuntu22.04-py3.11
      env:
        VLLM_LOGGING_LEVEL: ERROR
        VLLM_USE_MODELSCOPE: True
        TRANSFORMERS_OFFLINE: 1
    steps:
      - name: Checkout vllm-project/vllm-ascend repo
        uses: actions/checkout@v6

      - name: Prepare environment
        uses: ./.github/actions/e2e_test_pre
        with:
          vllm: ${{ inputs.vllm }}

      - name: Run vllm-project/vllm-ascend test
        env:
          PYTORCH_NPU_ALLOC_CONF: max_split_size_mb:256
          VLLM_WORKER_MULTIPROC_METHOD: spawn
        run: |
          pytest -sv --durations=0 tests/e2e/310p/test_offline_inference_310p.py

  e2e_310p-4cards:
    name: 310p multicards 4cards
    runs-on: linux-aarch64-310p-4
    if: ${{ inputs.contains_310 }}
    container:
      image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/cann:8.3.rc2-310p-ubuntu22.04-py3.11
      env:
        VLLM_LOGGING_LEVEL: ERROR
        VLLM_USE_MODELSCOPE: True
        TRANSFORMERS_OFFLINE: 1
    steps:
      - name: Checkout vllm-project/vllm-ascend repo
        uses: actions/checkout@v6

      - name: Prepare environment
        uses: ./.github/actions/e2e_test_pre
        with:
          vllm: ${{ inputs.vllm }}

      - name: Run vllm-project/vllm-ascend test
        env:
          PYTORCH_NPU_ALLOC_CONF: max_split_size_mb:256
          VLLM_WORKER_MULTIPROC_METHOD: spawn
        run: |
          pytest -sv --durations=0 tests/e2e/310p/test_offline_inference_parallel_310p.py