diff --git a/vllm/config.py b/vllm/config.py
index db35c848b..46d89e01e 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -2753,7 +2753,7 @@ class SpeculativeConfig:
         return self.num_speculative_tokens
 
     def use_eagle(self) -> bool:
-        return self.method in ("eagle", "eagle3", "deepseek_mtp")
+        return self.method in ("eagle", "eagle3")
 
     def __repr__(self) -> str:
         method = self.method
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index b25aa9ab2..81aa039bf 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -1330,6 +1330,7 @@ class EngineArgs:
         is_ngram_enabled = False
         is_eagle_enabled = False
         is_medusa_enabled = False
+        is_mtp_enabled = False
         if self.speculative_config is not None:
             # This is supported but experimental (handled below).
             speculative_method = self.speculative_config.get("method")
@@ -1338,13 +1339,15 @@ class EngineArgs:
                     is_ngram_enabled = True
                 elif speculative_method == "medusa":
                     is_medusa_enabled = True
-                elif speculative_method in ("eagle", "eagle3", "deepseek_mtp"):
+                elif speculative_method in ("eagle", "eagle3"):
                     is_eagle_enabled = True
+                elif speculative_method in ("deepseek_mtp",):
+                    is_mtp_enabled = True
             else:
                 speculative_model = self.speculative_config.get("model")
                 if speculative_model in ("ngram", "[ngram]"):
                     is_ngram_enabled = True
-            if not (is_ngram_enabled or is_eagle_enabled or is_medusa_enabled):
+            if not (is_ngram_enabled or is_eagle_enabled or is_medusa_enabled or is_mtp_enabled):
                 # Other speculative decoding methods are not supported yet.
                 _raise_or_fallback(feature_name="Speculative Decoding",
                                    recommend_to_remove=False)
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 3b1d3f94a..e5456a3c0 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -389,7 +389,7 @@ class Scheduler(SchedulerInterface):
                     # We use `request.num_tokens` instead of
                     # `request.num_prompt_tokens` to consider the resumed
                     # requests, which have output tokens.
-                    num_new_tokens = request.num_tokens - num_computed_tokens
+                    num_new_tokens = request.num_tokens_with_spec - num_computed_tokens
                     if (0 < self.scheduler_config.long_prefill_token_threshold
                             < num_new_tokens):
                         num_new_tokens = (
@@ -443,6 +443,15 @@ class Scheduler(SchedulerInterface):
                     structured_output_request_ids[
                         request.request_id] = req_index
                 req_index += 1
+                if request.spec_token_ids:
+                    num_scheduled_spec_tokens = (num_new_tokens +
+                                                    request.num_computed_tokens -
+                                                    request.num_tokens)
+                    if num_scheduled_spec_tokens > 0:
+                        # Trim spec_token_ids list to num_scheduled_spec_tokens.
+                        del request.spec_token_ids[num_scheduled_spec_tokens:]
+                        scheduled_spec_decode_tokens[request.request_id] = (
+                            request.spec_token_ids)
                 self.running.append(request)
                 if self.log_stats:
                     request.record_event(EngineCoreEventType.SCHEDULED,
@@ -983,7 +992,9 @@ class Scheduler(SchedulerInterface):
         num_computed_tokens = len(block_ids) * self.block_size
         if num_computed_tokens == request.num_tokens:
             num_computed_tokens -= 1
-
+        # due to no speculative trans from prefill node, push one manually
+        if self.vllm_config.speculative_config is not None:
+            request.spec_token_ids.extend([0] * self.vllm_config.speculative_config.num_speculative_tokens)
         # Update the request state for scheduling.
         request.num_computed_tokens = request.num_tokens - 1
 
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 910c0e80b..55b7ede65 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -158,6 +158,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             if get_pp_group().is_last_rank:
                 if self.speculative_config.method == "ngram":
                     self.drafter = NgramProposer(self.vllm_config)
+                elif self.speculative_config.method == "deepseek_mtp":
+                    self.drafter = None
                 elif self.speculative_config.use_eagle():
                     self.drafter = EagleProposer(self.vllm_config, self.device,
                                                  self)  # type: ignore
