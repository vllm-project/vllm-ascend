diff --git a/vllm/v1/core/sched/output.py b/vllm/v1/core/sched/output.py
index 257234430..de5a111e6 100644
--- a/vllm/v1/core/sched/output.py
+++ b/vllm/v1/core/sched/output.py
@@ -151,3 +151,5 @@ class SchedulerOutput:
 
     # KV Cache Connector metadata.
     kv_connector_metadata: Optional[KVConnectorMetadata] = None
+
+    num_step: Optional[int] = 1
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 5b54e6e26..370a0d982 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -197,6 +197,11 @@ class Scheduler(SchedulerInterface):
 
             num_new_tokens = (request.num_tokens_with_spec -
                               request.num_computed_tokens)
+
+            # In async schedule, the num_tokens_with_spec is updated behind schedule(). Here clear the incorrect value
+            if self.async_schedule and num_new_tokens < 0:
+                num_new_tokens = 0
+
             if (0 < self.scheduler_config.long_prefill_token_threshold <
                     num_new_tokens):
                 num_new_tokens = (
@@ -698,149 +703,162 @@ class Scheduler(SchedulerInterface):
         self,
         scheduler_output: SchedulerOutput,
         model_runner_output: ModelRunnerOutput,
+        num_steps = 1,
     ) -> EngineCoreOutputs:
-        sampled_token_ids = model_runner_output.sampled_token_ids
-        spec_token_ids = model_runner_output.spec_token_ids
-        logprobs = model_runner_output.logprobs
-        prompt_logprobs_dict = model_runner_output.prompt_logprobs_dict
+        cached_sampled_token_ids = model_runner_output.sampled_token_ids
+        cached_spec_token_ids = model_runner_output.spec_token_ids
+        cached_logprobs = model_runner_output.logprobs
+        cached_prompt_logprobs_dict = model_runner_output.prompt_logprobs_dict
         num_scheduled_tokens = scheduler_output.num_scheduled_tokens
 
-        new_running: list[Request] = []
         outputs: list[EngineCoreOutput] = []
         spec_decoding_stats: Optional[SpecDecodingStats] = None
 
-        # NOTE(woosuk): As len(self.running) can be up to 1K or more, the below
-        # loop can be a performance bottleneck. We should do our best to avoid
-        # expensive operations inside the loop.
-        for request in self.running:
-            req_id = request.request_id
-            num_tokens_scheduled = num_scheduled_tokens.get(req_id, 0)
-            if num_tokens_scheduled == 0:
-                # The request was not scheduled in this step.
-                new_running.append(request)
-                continue
+        for current_steps in range(num_steps):
+            new_running: list[Request] = []
+            if cached_sampled_token_ids == []:
+                sampled_token_ids = cached_sampled_token_ids
+                spec_token_ids = cached_spec_token_ids
+                logprobs = cached_logprobs
+                prompt_logprobs_dict = cached_prompt_logprobs_dict
+            else:
+                sampled_token_ids = cached_sampled_token_ids[current_steps]
+                spec_token_ids = cached_spec_token_ids[current_steps]
+                logprobs = cached_logprobs[current_steps]
+                prompt_logprobs_dict = cached_prompt_logprobs_dict[current_steps]
+
+            # NOTE(woosuk): As len(self.running) can be up to 1K or more, the below
+            # loop can be a performance bottleneck. We should do our best to avoid
+            # expensive operations inside the loop.
+            for request in self.running:
+                req_id = request.request_id
+                num_tokens_scheduled = num_scheduled_tokens.get(req_id, 0)
+                if num_tokens_scheduled == 0:
+                    # The request was not scheduled in this step.
+                    new_running.append(request)
+                    continue
 
-            req_index = model_runner_output.req_id_to_index[req_id]
-            generated_token_ids = sampled_token_ids[req_index]
-
-            scheduled_spec_token_ids = (
-                scheduler_output.scheduled_spec_decode_tokens.get(req_id))
-            if scheduled_spec_token_ids:
-                # num_computed_tokens represents the number of tokens
-                # processed in the current step, considering scheduled
-                # tokens and rejections. If some tokens are rejected,
-                # num_computed_tokens is decreased by the number of rejected
-                # tokens, where is given by:
-                # len(scheduled_spec_token_ids) + 1 - len(generated_token_ids).
-                num_tokens_rejected = (len(scheduled_spec_token_ids) + 1 -
-                                       len(generated_token_ids))
-                request.num_computed_tokens -= num_tokens_rejected
-                spec_decoding_stats = self.make_spec_decoding_stats(
-                    spec_decoding_stats,
-                    num_draft_tokens=len(scheduled_spec_token_ids),
-                    num_accepted_tokens=len(generated_token_ids) - 1)
-
-            cached_encoder_input_ids = (
-                self.encoder_cache_manager.get_cached_input_ids(request))
-            # OPTIMIZATION: Avoid list(set) if the set is empty.
-            if cached_encoder_input_ids:
-                for input_id in list(cached_encoder_input_ids):
-                    mm_positions = request.mm_positions[input_id]
-                    start_pos = mm_positions.offset
-                    num_tokens = mm_positions.length
-                    if start_pos + num_tokens <= request.num_computed_tokens:
-                        # The encoder output is already processed and stored
-                        # in the decoder's KV cache.
-                        self.encoder_cache_manager.free_encoder_input(
-                            request, input_id)
-
-            stopped = False
-            new_logprobs = None
-            new_token_ids = generated_token_ids
-            kv_transfer_params = None
-
-            # Append generated tokens and check for stop. Note that if
-            # a request is still being prefilled, we expect the model runner
-            # to return empty token ids for the request.
-            for num_new, output_token_id in enumerate(new_token_ids, 1):
-                request.append_output_token_ids(output_token_id)
-
-                # Check for stop and update request state.
-                # This must be called before we make the EngineCoreOutput.
-                stopped = check_stop(request, self.max_model_len)
-                if stopped:
-                    kv_transfer_params = self._free_request(request)
-                    del new_token_ids[num_new:]  # Trim new tokens if needed.
-                    break
+                req_index = model_runner_output.req_id_to_index[req_id]
+                generated_token_ids = sampled_token_ids[req_index]
+
+                scheduled_spec_token_ids = (
+                    scheduler_output.scheduled_spec_decode_tokens.get(req_id))
+                if scheduled_spec_token_ids:
+                    # num_computed_tokens represents the number of tokens
+                    # processed in the current step, considering scheduled
+                    # tokens and rejections. If some tokens are rejected,
+                    # num_computed_tokens is decreased by the number of rejected
+                    # tokens, where is given by:
+                    # len(scheduled_spec_token_ids) + 1 - len(generated_token_ids).
+                    num_tokens_rejected = (len(scheduled_spec_token_ids) + 1 -
+                                           len(generated_token_ids))
+                    request.num_computed_tokens -= num_tokens_rejected
+                    spec_decoding_stats = self.make_spec_decoding_stats(
+                        spec_decoding_stats,
+                        num_draft_tokens=len(scheduled_spec_token_ids),
+                        num_accepted_tokens=len(generated_token_ids) - 1)
+
+                cached_encoder_input_ids = (
+                    self.encoder_cache_manager.get_cached_input_ids(request))
+                # OPTIMIZATION: Avoid list(set) if the set is empty.
+                if cached_encoder_input_ids:
+                    for input_id in list(cached_encoder_input_ids):
+                        mm_positions = request.mm_positions[input_id]
+                        start_pos = mm_positions.offset
+                        num_tokens = mm_positions.length
+                        if start_pos + num_tokens <= request.num_computed_tokens:
+                            # The encoder output is already processed and stored
+                            # in the decoder's KV cache.
+                            self.encoder_cache_manager.free_encoder_input(
+                                request, input_id)
+
+                stopped = False
+                new_logprobs = None
+                new_token_ids = generated_token_ids
+                kv_transfer_params = None
+
+                # Append generated tokens and check for stop. Note that if
+                # a request is still being prefilled, we expect the model runner
+                # to return empty token ids for the request.
+                for num_new, output_token_id in enumerate(new_token_ids, 1):
+                    request.append_output_token_ids(output_token_id)
+
+                    # Check for stop and update request state.
+                    # This must be called before we make the EngineCoreOutput.
+                    stopped = check_stop(request, self.max_model_len)
+                    if stopped:
+                        kv_transfer_params = self._free_request(request)
+                        del new_token_ids[num_new:]  # Trim new tokens if needed.
+                        break
 
-            # Extract sample logprobs if needed.
-            if request.sampling_params.logprobs is not None and logprobs:
-                # NOTE: once we support N tokens per step (spec decode),
-                # the outer lists can be of length > 1.
-                new_logprobs = logprobs.slice(req_index, req_index + 1)
-
-            if new_token_ids and self.structured_output_manager.should_advance(
-                    request):
-                # NOTE: structured_output_request
-                # should not be None if use_structured_output, we have
-                # check above, so safe to ignore type warning
-                request.structured_output_request.grammar.accept_tokens(  # type: ignore[union-attr]
-                    req_id, new_token_ids)
-
-            # Add newly generated spec token ids to the request.
-            if spec_token_ids is not None:
-                if self.structured_output_manager.should_advance(request):
-                    metadata = request.structured_output_request
-                    # Needs to happen after new_token_ids are accepted.
-                    request.spec_token_ids = metadata.grammar.validate_tokens(  # type: ignore[union-attr]
-                        spec_token_ids[req_index])
-                else:
-                    request.spec_token_ids = spec_token_ids[req_index]
-
-            # Get prompt logprobs for this request.
-            prompt_logprobs_tensors = prompt_logprobs_dict.get(req_id)
-            if new_token_ids or kv_transfer_params:
-
-                # Add EngineCoreOutput for this Request.
-                outputs.append(
-                    EngineCoreOutput(
-                        request_id=req_id,
-                        new_token_ids=new_token_ids,
-                        finish_reason=request.get_finished_reason(),
-                        new_logprobs=new_logprobs,
-                        new_prompt_logprobs_tensors=prompt_logprobs_tensors,
-                        stop_reason=request.stop_reason,
-                        events=request.take_events(),
-                        kv_transfer_params=kv_transfer_params,
-                    ))
+                # Extract sample logprobs if needed.
+                if request.sampling_params.logprobs is not None and logprobs:
+                    # NOTE: once we support N tokens per step (spec decode),
+                    # the outer lists can be of length > 1.
+                    new_logprobs = logprobs.slice(req_index, req_index + 1)
+
+                if new_token_ids and self.structured_output_manager.should_advance(
+                        request):
+                    # NOTE: structured_output_request
+                    # should not be None if use_structured_output, we have
+                    # check above, so safe to ignore type warning
+                    request.structured_output_request.grammar.accept_tokens(  # type: ignore[union-attr]
+                        req_id, new_token_ids)
+
+                # Add newly generated spec token ids to the request.
+                if spec_token_ids is not None:
+                    if self.structured_output_manager.should_advance(request):
+                        metadata = request.structured_output_request
+                        # Needs to happen after new_token_ids are accepted.
+                        request.spec_token_ids = metadata.grammar.validate_tokens(  # type: ignore[union-attr]
+                            spec_token_ids[req_index])
+                    else:
+                        request.spec_token_ids = spec_token_ids[req_index]
+
+                # Get prompt logprobs for this request.
+                prompt_logprobs_tensors = prompt_logprobs_dict.get(req_id)
+                if new_token_ids or kv_transfer_params:
+
+                    # Add EngineCoreOutput for this Request.
+                    outputs.append(
+                        EngineCoreOutput(
+                            request_id=req_id,
+                            new_token_ids=new_token_ids,
+                            finish_reason=request.get_finished_reason(),
+                            new_logprobs=new_logprobs,
+                            new_prompt_logprobs_tensors=prompt_logprobs_tensors,
+                            stop_reason=request.stop_reason,
+                            events=request.take_events(),
+                            kv_transfer_params=kv_transfer_params,
+                        ))
 
-            else:
-                # Invariant: EngineCore returns no partial prefill outputs.
-                assert not prompt_logprobs_tensors
-
-            if not stopped:
-                new_running.append(request)
-
-        # P/D: update state for finished KV Transfers.
-        self._update_from_kv_xfer_finished(model_runner_output)
-
-        # Return the cached request data to the queue so they can be reused.
-        for req_data in scheduler_output.scheduled_cached_reqs:
-            # NOTE(rob): since we free stopped reqs above, adding stopped reqs
-            # to _cached_reqs_data will cause a memory leak.
-            if req_data.req_id not in self.finished_req_ids:
-                self._cached_reqs_data[req_data.req_id].append(req_data)
-
-        # switch the self.make_stats() to before self.running = new_running, to make the statistic of running request accurate
-        engine_core_outputs = EngineCoreOutputs(
-            outputs=outputs,
-            scheduler_stats=self.make_stats(spec_decoding_stats),
-        )
-        self.running = new_running
-        if self.include_finished_set:
-            #TODO currently sending duplicates here, improve this
-            engine_core_outputs.finished_requests = (
-                scheduler_output.finished_req_ids | self.finished_req_ids)
+                else:
+                    # Invariant: EngineCore returns no partial prefill outputs.
+                    assert not prompt_logprobs_tensors
+
+                if not stopped:
+                    new_running.append(request)
+
+            # P/D: update state for finished KV Transfers.
+            self._update_from_kv_xfer_finished(model_runner_output)
+
+            # Return the cached request data to the queue so they can be reused.
+            for req_data in scheduler_output.scheduled_cached_reqs:
+                # NOTE(rob): since we free stopped reqs above, adding stopped reqs
+                # to _cached_reqs_data will cause a memory leak.
+                if req_data.req_id not in self.finished_req_ids:
+                    self._cached_reqs_data[req_data.req_id].append(req_data)
+
+            # switch the self.make_stats() to before self.running = new_running, to make the statistic of running request accurate
+            engine_core_outputs = EngineCoreOutputs(
+                outputs=outputs,
+                scheduler_stats=self.make_stats(spec_decoding_stats),
+            )
+            self.running = new_running
+            if self.include_finished_set:
+                #TODO currently sending duplicates here, improve this
+                engine_core_outputs.finished_requests = (
+                    scheduler_output.finished_req_ids | self.finished_req_ids)
 
         return engine_core_outputs
 
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 900786c4d..ffb80f4db 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -97,10 +97,14 @@ class EngineCore:
                 vllm_config.scheduler_config.scheduler_cls)
         additional_config = vllm_config.additional_config
         self.async_schedule = False
+        self.step_num = 1
         if additional_config:
             self.async_schedule = additional_config.get(
                     "async_schedule", False)
 
+            if additional_config.get("multi_step", False):
+                self.step_num = 4
+
         self.scheduler: SchedulerInterface = Scheduler(
             vllm_config=vllm_config,
             kv_cache_config=kv_cache_config,
@@ -109,7 +113,7 @@ class EngineCore:
             > 1,
             log_stats=self.log_stats,
         )
-
+        self.scheduler.num_lookahead_tokens = (self.step_num - 1) * (1 + self.scheduler.num_spec_tokens)
         # Setup MM Input Mapper.
         self.mm_input_cache_server = MirroredProcessingCache(
             vllm_config.model_config)
@@ -249,9 +253,12 @@ class EngineCore:
                 scheduler_stats=self.scheduler.make_stats(),
             )
         scheduler_output = self.scheduler.schedule()
+        scheduler_output.num_step = self.step_num
+        for req_id, num_scheduled_token in scheduler_output.num_scheduled_tokens.items():
+            self.scheduler.requests[req_id].num_computed_tokens += num_scheduled_token * (self.step_num - 1)
         model_output = self.execute_model(scheduler_output)
         engine_core_outputs = self.scheduler.update_from_output(
-            scheduler_output, model_output)  # type: ignore
+            scheduler_output, model_output, num_steps=self.step_num)  # type: ignore
 
         return engine_core_outputs
 
@@ -274,26 +281,32 @@ class EngineCore:
                 scheduler_stats=self.scheduler.make_stats(),
             )
         self.cur_batch = self.scheduler.schedule()
+        self.cur_batch.num_step = self.step_num
+        for req_id, num_scheduled_token in self.cur_batch.num_scheduled_tokens.items():
+            self.scheduler.requests[req_id].num_computed_tokens += num_scheduled_token * (self.step_num - 1)
 
         output = None
         if self._slow_future is not None:
             output = self._slow_future.result()
             for each_cached_req in self.cur_batch.scheduled_cached_reqs:
                 if each_cached_req.req_id in output.req_ids:
-                    req_id = output.req_id_to_index[each_cached_req.req_id]
-                    new_tokens = output.sampled_token_ids[req_id]
+                    req_idx = output.req_id_to_index[each_cached_req.req_id]
+                    new_tokens = output.sampled_token_ids[-1][req_idx]
                     spec_tokens = None
                     len_spec_tokens = 0
                     if output.spec_token_ids is not None:
-                        spec_tokens = output.spec_token_ids[req_id]
+                        spec_tokens = output.spec_token_ids[-1][req_idx]
                         len_spec_tokens = len(spec_tokens)
                     if len(each_cached_req.new_token_ids) != self.cur_batch.num_scheduled_tokens[each_cached_req.req_id]:
                         self.cur_batch.total_num_scheduled_tokens = self.cur_batch.total_num_scheduled_tokens - self.cur_batch.num_scheduled_tokens[each_cached_req.req_id] + len(new_tokens[-1:]) + len_spec_tokens
                         self.cur_batch.num_scheduled_tokens[each_cached_req.req_id] = len(new_tokens[-1:]) + len_spec_tokens
                         each_cached_req.new_token_ids = new_tokens[-1:]
+                        actual_computed_tokens = 0
+                        for i in range(self.step_num):
+                            actual_computed_tokens += len(output.sampled_token_ids[i][req_idx])
                         each_cached_req.num_computed_tokens =  \
-                            each_cached_req.num_computed_tokens + len(new_tokens) \
-                            - self.last_batch.num_scheduled_tokens[each_cached_req.req_id]
+                            each_cached_req.num_computed_tokens + actual_computed_tokens \
+                            - self.last_batch.num_scheduled_tokens[each_cached_req.req_id] * self.step_num
                         if spec_tokens is not None:
                             self.cur_batch.scheduled_spec_decode_tokens[each_cached_req.req_id] = spec_tokens[:]
 
@@ -301,7 +314,7 @@ class EngineCore:
         time.sleep(0.005) # 5ms
 
         if output is not None:
-            engine_core_outputs = self.scheduler.update_from_output(self.last_batch, output)
+            engine_core_outputs = self.scheduler.update_from_output(self.last_batch, output, num_steps=self.cur_batch.num_step)
 
         self.last_batch = self.cur_batch
         # By wcd, return None or not?
