#
# Copyright (c) 2025 Huawei Technologies Co., Ltd. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# This file is a part of the omni project.
#

ARG BASE_IMAGE
FROM ${BASE_IMAGE}

ARG PIP_INDEX_URL="http://mirrors.tools.huawei.com/pypi/simple"
ARG PIP_TRUSTED_HOST="mirrors.tools.huawei.com"
WORKDIR /workspace

COPY omni_infer /workspace/omni_infer
# Download and install omni_infer vllm
RUN python3 -m pip uninstall -y vllm && \
    python3 -m pip uninstall -y vllm_ascend

RUN unset http_proxy https_proxy && \
    pip install -i ${PIP_INDEX_URL} --trusted-host ${PIP_TRUSTED_HOST} ml-dtypes tornado compressed-tensors==0.9.4 torch==2.5.1 build pytz pybind11
RUN cd omni_infer/infer_engines && \
    bash bash_install_code.sh
# install nginx
COPY nginx /workspace/nginx
RUN cd /workspace/nginx && \
    tar -zxf nginx-1.24.0.tar.gz && \
    cd nginx-1.24.0 && \
    export NGINX_PATH=/usr/local/nginx && \
    export PATH=$PATH:$NGINX_PATH && \
    CFLAGS="-O2"  ./configure --sbin-path=${NGINX_PATH} --add-dynamic-module=/workspace/omni_infer/omni/accelerators/sched/global_proxy/modules/ngx_http_prefill_module --add-dynamic-module=/workspace/omni_infer/omni/accelerators/sched/global_proxy/modules/ngx_http_set_request_id_module --add-dynamic-module=/workspace/omni_infer/omni/accelerators/sched/global_proxy/modules/ngx_http_upstream_length_balance_module --add-dynamic-module=/workspace/omni_infer/omni/accelerators/sched/global_proxy/lb_sdk/modules/ngx_http_upstream_greedy_timeout_module --add-dynamic-module=/workspace/omni_infer/omni/accelerators/sched/global_proxy/lb_sdk/modules/ngx_http_upstream_prefill_score_balance_module --add-dynamic-module=/workspace/omni_infer/omni/accelerators/sched/global_proxy/lb_sdk/modules/ngx_http_upstream_weighted_least_active_module --add-dynamic-module=/workspace/omni_infer/omni/accelerators/sched/global_proxy/lb_sdk/modules/ngx_http_upstream_pd_score_balance_module --without-http_gzip_module --with-ld-opt="-lm" && \
    CFLAGS="-O2"  make -j && make install && ln -s ${NGINX_PATH}/nginx /usr/local/bin/nginx

# install vllm omni
RUN unset http_proxy https_proxy && \
    cd /workspace/omni_infer/infer_engines/vllm && \
    sed -i 's/^xgrammar[[:space:]]*==[[:space:]]*0.1.19/xgrammar == 0.1.18/' requirements/common.txt && \
    VLLM_TARGET_DEVICE="empty" pip install -i ${PIP_INDEX_URL} --trusted-host ${PIP_TRUSTED_HOST} -e . && \
    source ~/.bashrc && \
    cd /workspace/omni_infer && \
    pip install -i ${PIP_INDEX_URL} --trusted-host ${PIP_TRUSTED_HOST} -e .

COPY pytorch_v2.5.1_py311.tar.gz /workspace
RUN pip uninstall -y torch-npu && \
    cd /workspace && \
    mkdir pta && cd pta && \
    mv ../pytorch_v2.5.1_py311.tar.gz . && \
    tar -zxvf pytorch_v2.5.1_py311.tar.gz && \
    pip install -i ${PIP_INDEX_URL} --trusted-host ${PIP_TRUSTED_HOST} --proxy="" ./torch_npu-*_aarch64.whl && \
    cd .. && rm -rf pta

RUN unset http_proxy https_proxy

CMD ["/bin/bash"]
