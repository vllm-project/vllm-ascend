#
# Copyright (c) 2025 Huawei Technologies Co., Ltd. All Rights Reserved.
# This file is a part of the vllm-ascend project.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

from typing import Generator, List, Tuple

import torch
import torch_npu
from vllm.attention.selector import get_attn_backend
from vllm.config import CacheConfig, DeviceConfig, ModelConfig, ParallelConfig
from vllm.distributed import (get_tensor_model_parallel_rank,
                              get_tensor_model_parallel_world_size)
from vllm.utils import LayerBlockType

TYPE_QUANT_QKV_ONLINE = 3

SRC_DTYPE_TO_ACL_DTYPE = {
    torch.float16: 1,
    torch.bfloat16: 27,
}


def quant_per_tensor(in_tensor: torch.Tensor,
                     input_scale: torch.Tensor,
                     input_offset: torch.Tensor,
                     function=False):
    input_scale = input_scale.view(-1)
    input_offset = input_offset.view(-1)
    return torch_npu.npu_quantize(in_tensor, input_scale, input_offset,
                                  torch.qint8, -1, function)


def wrapper_weights_iterator(func):

    def _safetensors_weights_iterator(
        hf_weights_files: List[str],
        use_tqdm_on_load: bool,
    ) -> Generator[Tuple[str, torch.Tensor], None, None]:
        current_rank = get_tensor_model_parallel_rank()
        world_size = get_tensor_model_parallel_world_size()
        for name, weight in func(hf_weights_files, use_tqdm_on_load):
            # The name of attention weights generated by msmodelslim
            # must be modified so that these weights can be loaded
            # into Attention module rather than LlamaAttention module.
            if "fa_" in name and ".attn." not in name:
                name = name.split(".")
                name.insert(name.index("self_attn") + 1, "attn")
                name = ".".join(name)
                # vLLM originally does not support splitting attention
                # weights with respect to TP ranks. We need split
                # weights manually here.
                if world_size <= 0:
                    raise ValueError(
                        "Expected world_size should be greater than 0"
                        f"but got {world_size}.")
                split_size = weight.size(0) // world_size
                weight = weight[current_rank * split_size:(current_rank + 1) *
                                split_size]

            yield name, weight

    return _safetensors_weights_iterator


# Replace CacheEngine.__init__
# vLLM does not include int8 cache dtype.
# We should set it here.
def cache_engine_init(
    self,
    cache_config: CacheConfig,
    model_config: ModelConfig,
    parallel_config: ParallelConfig,
    device_config: DeviceConfig,
) -> None:
    self.cache_config = cache_config
    self.model_config = model_config
    self.parallel_config = parallel_config
    self.device_config = device_config

    self.head_size = model_config.get_head_size()
    # Models like Jamba, have mixed typed layers, E.g Mamba
    self.num_attention_layers = model_config.get_num_layers_by_block_type(
        parallel_config, LayerBlockType.attention)
    self.num_kv_heads = model_config.get_num_kv_heads(parallel_config)

    self.block_size = cache_config.block_size
    self.num_gpu_blocks = cache_config.num_gpu_blocks
    if self.num_gpu_blocks:
        self.num_gpu_blocks //= parallel_config.pipeline_parallel_size
    self.num_cpu_blocks = cache_config.num_cpu_blocks
    if self.num_cpu_blocks:
        self.num_cpu_blocks //= parallel_config.pipeline_parallel_size

    # modified here. vLLM does not include int8 cache dtype.
    # We should set it here.
    self.dtype = torch.int8

    # Get attention backend.
    self.attn_backend = get_attn_backend(self.head_size,
                                         model_config.dtype,
                                         cache_config.cache_dtype,
                                         self.block_size,
                                         model_config.is_attention_free,
                                         use_mla=model_config.use_mla)

    # Initialize the cache.
    self.gpu_cache = self._allocate_kv_cache(self.num_gpu_blocks,
                                             self.device_config.device_type)
    self.cpu_cache = self._allocate_kv_cache(self.num_cpu_blocks, "cpu")
